{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ucto\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can enter the parts that you want to run. The identifiers are the run names, and you can find them further\n",
    "# in the notebook. Use \"all\" to run all of them; this option overrules all other run names. Also note that it takes a\n",
    "# seriously large amount of time to run them all.\n",
    "\n",
    "_run = set([])\n",
    "_use_subset = False\n",
    "_show_graphics = True\n",
    "\n",
    "def do_run(name):\n",
    "    return name in _run or \"all\" in _run\n",
    "\n",
    "_run_stats = {}\n",
    "\n",
    "def register_run(name, description, params):\n",
    "    if name not in _run_stats:\n",
    "        _run_stats[name] = {}\n",
    "        _run_stats[name]['description'] = description\n",
    "        _run_stats[name]['params'] = {}\n",
    "        for param in params:\n",
    "            _run_stats[name]['params'][param] = ''\n",
    "        \n",
    "        _run_stats[name]['best'] = 0\n",
    "        _run_stats[name]['best_predictions'] = []\n",
    "        _run_stats[name]['orig_predictions'] = []\n",
    "        _run_stats[name]['history'] = []\n",
    "\n",
    "def update_stats(name, y_test, y_pred, params):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    _run_stats[name]['history'].append(accuracy)\n",
    "    if accuracy > _run_stats[name]['best']:\n",
    "        _run_stats[name]['best'] = accuracy\n",
    "        for param in params:\n",
    "            _run_stats[name]['params'][param] = params[param]\n",
    "        _run_stats[name]['best_predictions'] = y_pred\n",
    "        _run_stats[name]['orig_predictions'] = y_test\n",
    "        \n",
    "def summarise_run(name):\n",
    "    if name not in _run_stats:\n",
    "        print(\"Name %s is not registrered.\" % (name))\n",
    "        return\n",
    "    \n",
    "    print(\"Model:    \\n\\t%s\" % (name))\n",
    "    print(\"Settings: \\n\\t%s\" % (_run_stats[name]['params']))\n",
    "    print(\"Accuracy: \\n\\t%s\" % (_run_stats[name]['best']))\n",
    "    \n",
    "    cm = confusion_matrix(_run_stats[name]['orig_predictions'], _run_stats[name]['best_predictions'])\n",
    "    \n",
    "    if _show_graphics:\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm, classes=[\"BEL\", \"DUT\"], title=name + \" \" + str(_run_stats[name]['params']))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(cm)       \n",
    "\n",
    "def summarise_all():\n",
    "    for name in _run_stats.keys():\n",
    "        summarise_run(name)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "import sys\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # running in console\n",
    "    _show_graphics = False\n",
    "    from tqdm import tqdm as tqdm, trange as tnrange\n",
    "    \n",
    "    for v in sys.argv:\n",
    "        if v.startswith(\"r:\"):\n",
    "            _run.add(v[2:])\n",
    "        elif v == \"o:subset\":\n",
    "            _use_subset = True\n",
    "else:\n",
    "    # running in notebook\n",
    "    from tqdm import tqdm_notebook as tqdm, tnrange as tnrange\n",
    "    \n",
    "    _use_subset = True\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(file):\n",
    "    with open(file) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucto_config = \"tokconfig-nld\"\n",
    "tokeniser = ucto.Tokenizer(ucto_config, sentenceperlineinput=True, sentencedetection=False, paragraphdetection=False)\n",
    "\n",
    "# We read the file with ucto and tokenise it according to its default Dutch tokenisation scheme, which is rule-based\n",
    "# and definitely better than a plain whitespace tokeniser from sklearn. Afterwards we concatenate the tokens back to a \n",
    "# whitespace seperated line, which can then be normally processed with sklearn's tokenisers.\n",
    "def read_data(file):\n",
    "    text = {}\n",
    "    with open(file) as f:\n",
    "        for line in tqdm(f):\n",
    "            sentence, language = line.strip().split(\"\\t\")\n",
    "            tokeniser.process(sentence)\n",
    "\n",
    "            if language not in text:\n",
    "                text[language] = []\n",
    "\n",
    "            current_line = []\n",
    "            for token in tokeniser:\n",
    "                current_line.append(str(token))\n",
    "                if token.isendofsentence():\n",
    "                    #print(current_line)\n",
    "                    text[language].append(\" \".join(current_line))\n",
    "                    current_line = []\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading development set from pickle.\n",
      "development set\n",
      "\t LAN\t size \t avg length\n",
      "\t BEL \t 250 \t 40.456\n",
      "\t DUT \t 250 \t 40.088\n",
      "Done reading training set from pickle.\n",
      "training set\n",
      "\t LAN\t size \t avg length\n",
      "\t BEL \t 150000 \t 40.273626666666665\n",
      "\t DUT \t 150000 \t 40.37152\n"
     ]
    }
   ],
   "source": [
    "# If this is the first run, then we have to tokenise the text. In other cases we probably have saved a pickled version\n",
    "# somewhere. If not, we will tokenise the text anyway. No worries.\n",
    "\n",
    "# First the development set\n",
    "try:\n",
    "    with open('data/dev.txt.pickle', 'rb') as f:\n",
    "        _l_dev_text = pickle.load(f)\n",
    "        print(\"Done reading development set from pickle.\")\n",
    "except IOError:\n",
    "    _l_dev_text = read_data('data/dev.txt')\n",
    "    print(\"Done tokenising development set.\")\n",
    "    with open('data/dev.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_dev_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing development set from pickle.\")\n",
    "\n",
    "print(\"development set\")\n",
    "print(\"\\t LAN\\t size \\t avg length\")\n",
    "for l in _l_dev_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_dev_text[l]), \"\\t\", sum([len(x.split()) for x in _l_dev_text[l]])/len(_l_dev_text[l]))\n",
    "\n",
    "# And then the training set. This takes bit more time...\n",
    "try:\n",
    "    with open('data/train.txt.pickle', 'rb') as f:\n",
    "        _l_trn_text = pickle.load(f)\n",
    "        print(\"Done reading training set from pickle.\")\n",
    "except IOError:\n",
    "    _l_trn_text = read_data('data/train.txt')\n",
    "    print(\"Done tokenising training set.\")\n",
    "    with open('data/train.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_trn_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing training set from pickle.\")\n",
    "\n",
    "print(\"training set\")\n",
    "print(\"\\t LAN\\t size \\t avg length\")\n",
    "for l in _l_trn_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_trn_text[l]), \"\\t\", sum([len(x.split()) for x in _l_trn_text[l]])/len(_l_trn_text[l]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert the training and development material into the right shape, and make sure that we also keep track of\n",
    "# the labels.\n",
    "\n",
    "X_training = []\n",
    "y_training = []\n",
    "for l in _l_trn_text.keys():\n",
    "    for s in _l_trn_text[l]:\n",
    "        X_training.append(s)\n",
    "        y_training.append(l)\n",
    "X_training = np.array(X_training)\n",
    "y_training = np.array(y_training)\n",
    "\n",
    "\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "for l in _l_dev_text.keys():\n",
    "    for s in _l_dev_text[l]:\n",
    "        X_dev.append(s)\n",
    "        y_dev.append(l)\n",
    "X_dev = np.array(X_dev)\n",
    "y_dev = np.array(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training subset\n"
     ]
    }
   ],
   "source": [
    "# Sometimes for testing whether some code words, you might want to use a subset. Use this one. Or another one. I don't\n",
    "# care. \n",
    "\n",
    "import random\n",
    "#use = random.sample(range(1, 299999), 100000)\n",
    "use = random.sample(range(1, 299999), 10000)\n",
    "\n",
    "if _use_subset:   \n",
    "    X_training = X_training[use]\n",
    "    y_training = y_training[use]\n",
    "    \n",
    "    print(\"training subset\")\n",
    "    #print(\"\\t LAN\\t size \\t avg length\")\n",
    "    #for l in _l_trn_text.keys():\n",
    "    #    print(\"\\t\", l, \"\\t\", len(_l_trn_text[l]), \"\\t\", sum([len(x.split()) for x in _l_trn_text[l]])/len(_l_trn_text[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if do_run(\"svc2\"):\n",
    "\n",
    "    register_run(\"svc2\",\n",
    "                 \"character and word n-grams, with count feature vectors, used in a linear support vector classifier setting\",\n",
    "                 ['min_cn', 'max_cn', 'min_n', 'max_n'])\n",
    "\n",
    "    \n",
    "    for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "        for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "            \n",
    "            for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "                for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "                    \n",
    "                    steps = [('char', CountVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                             ('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                            ]\n",
    "\n",
    "                    union = FeatureUnion(steps)\n",
    "\n",
    "                    pipeline = Pipeline([\n",
    "                        ('union', union),\n",
    "                        ('svc', SVC(kernel='linear')),\n",
    "                    ])\n",
    "\n",
    "                    model = pipeline.fit(X_training, y_training)\n",
    "                    y_pred = model.predict(X_dev)\n",
    "\n",
    "                    update_stats(\"svc2\", y_dev, y_pred, {'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n",
    "    #summarise_run(\"svc2\", y_dev, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "if do_run(\"svc1\"):\n",
    "\n",
    "    register_run(\"svc1\",\n",
    "                 \"character and word n-grams, with tf-idf feature vectors, used in a linear support vector classifier setting\",\n",
    "                 ['min_cn', 'max_cn', 'min_n', 'max_n'])\n",
    "\n",
    "\n",
    "    for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "        for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "            \n",
    "            for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "                for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "                    \n",
    "                    steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                             ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                            ]\n",
    "\n",
    "                    union = FeatureUnion(steps)\n",
    "\n",
    "                    pipeline = Pipeline([\n",
    "                        ('union', union),\n",
    "                        ('svc', SVC(kernel='linear')),\n",
    "                    ])\n",
    "\n",
    "                    prediction = pipeline.fit(X_training, y_training)\n",
    "                    score = prediction.score(X_dev, y_dev)\n",
    "\n",
    "                    model = pipeline.fit(X_training, y_training)\n",
    "                    y_pred = model.predict(X_dev)\n",
    "\n",
    "                    update_stats(\"svc1\", y_dev, y_pred, {'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "if do_run(\"mnb1\"):\n",
    "    \n",
    "    register_run(\"mnb1\",\n",
    "             \"character and word n-grams, with tf-idf feature vectors, used in a multinominal naive bayes setting\",\n",
    "             ['min_cn', 'max_cn', 'min_n', 'max_n'])\n",
    "    \n",
    "    for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "        for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "            \n",
    "            for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "                for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "                    \n",
    "                    steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                             ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                            ]\n",
    "\n",
    "                    union = FeatureUnion(steps)\n",
    "\n",
    "                    pipeline = Pipeline([\n",
    "                        ('union', union),\n",
    "                        ('mnb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "                    model = pipeline.fit(X_training, y_training)\n",
    "                    y_pred = model.predict(X_dev)\n",
    "\n",
    "                    update_stats(\"mnb1\", y_dev, y_pred, {'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "if do_run(\"knn1\"):\n",
    "    \n",
    "    register_run(\"knn1\",\n",
    "             \"character and word n-grams, with tf-idf feature vectors, used in a k nearest neighbours setting\",\n",
    "             ['neighbours', 'min_cn', 'max_cn', 'min_n', 'max_n'])\n",
    "    \n",
    "    for neighbours in tnrange(1,7, desc=\"neighbours\"):\n",
    "\n",
    "        for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "            for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "\n",
    "                for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "                    for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "\n",
    "                        steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                                 ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                                ]\n",
    "\n",
    "                        union = FeatureUnion(steps)\n",
    "\n",
    "                        pipeline = Pipeline([\n",
    "                            ('union', union),\n",
    "                            ('mnb', KNeighborsClassifier(n_neighbors=neighbours)),\n",
    "                        ])\n",
    "\n",
    "                        model = pipeline.fit(X_training, y_training)\n",
    "                        y_pred = model.predict(X_dev)\n",
    "\n",
    "                        update_stats(\"knn1\", y_dev, y_pred, {'neighbours': neighbours, 'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "if do_run(\"fat1\"):\n",
    "    \n",
    "    register_run(\"fat1\",\n",
    "             \"character and word n-grams, with embeddings, used in a fasttext (w2v sg) setting\",\n",
    "             [])\n",
    "\n",
    "    with open('fasttext.train.txt', 'w') as f:\n",
    "        for line, label in zip(X_training, y_training):\n",
    "            f.write(line + \" __language__\" + label + \"\\n\")\n",
    "\n",
    "    ft_classifier = fasttext.supervised('fasttext.train.txt', 'model', \n",
    "                                        min_count=1, \n",
    "                                        word_ngrams=3, \n",
    "                                        minn=7, \n",
    "                                        maxn=7, \n",
    "                                        thread=2, \n",
    "                                        label_prefix='__language__')\n",
    "    ft_predictions = ft_classifier.predict(X_dev)\n",
    "    \n",
    "    update_stats(\"fat1\", y_dev, ft_predictions, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a new vectoriser based on the loglikelihood values as computed by colibricore-loglikelihood.\n",
    "# Based on some thresholds (n-gram occurrence >= 2, 1 <= n <= 3 -grams), it computes the occurrence counts,\n",
    "# their frequency and the corresponding loglikelihood scores.\n",
    "# These scores are sorted, and then this vectoriser takes the top m patterns, and marks whether that pattern\n",
    "# is presented in the given set.\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LLHbasedBinaryVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, count=1000):\n",
    "        self.llh_1000 = []\n",
    "        with open('data/DUT_BEL.t2m1l3.llh.top1000', 'r') as f:\n",
    "            for n, line in enumerate(f):\n",
    "                self.llh_1000.append(line.split(\"\\t\")[0])\n",
    "                if n >= count:\n",
    "                    break\n",
    "    \n",
    "    def llh_binary_countvectorizer(self, line):\n",
    "        values = []\n",
    "        for k in self.llh_1000:\n",
    "            values.append(1*(k in line))\n",
    "        return values\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        result = []\n",
    "        for l in df:\n",
    "            result.append(self.llh_binary_countvectorizer(l))\n",
    "        return result\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if do_run(\"svc3\"):\n",
    "    \n",
    "    register_run(\"svc3\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a linear support vector classifier\",\n",
    "             ['count'])\n",
    "    \n",
    "    for count in tqdm(range(1, 10000, 500), desc=\"# llh counts\"):\n",
    "\n",
    "        steps = [('llh', LLHbasedBinaryVectorizer(count=count)),\n",
    "                 #('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                ]\n",
    "\n",
    "        union = FeatureUnion(steps)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('union', union),\n",
    "            ('svc', SVC(kernel='linear')),\n",
    "        ])\n",
    "\n",
    "        model = pipeline.fit(X_training, y_training)\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(\"svc3\", y_dev, y_pred, {'count': count})\n",
    "\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if do_run(\"mnb2\"):\n",
    "    \n",
    "    register_run(\"mnb2\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a multinominal naive bayes setting\",\n",
    "             ['count'])\n",
    "    \n",
    "    for count in tqdm(range(1, 10000, 500), desc=\"# llh counts\"):\n",
    "\n",
    "        steps = [('llh', LLHbasedBinaryVectorizer(count=count)),\n",
    "                 #('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                ]\n",
    "\n",
    "        union = FeatureUnion(steps)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('union', union),\n",
    "            ('mnb', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "        model = pipeline.fit(X_training, y_training)\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(\"mnb2\", y_dev, y_pred, {'count': count})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if do_run(\"knn2\"):\n",
    "    \n",
    "    register_run(\"knn2\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a k nearest neighbours setting\",\n",
    "             ['neighbours', 'count'])\n",
    "    \n",
    "    for neighbours in tnrange(1,7, desc=\"neighbours\"):\n",
    "        for count in tqdm(range(1, 10000, 500), desc=\"# llh counts\"):\n",
    "\n",
    "            steps = [('llh', LLHbasedBinaryVectorizer(count=count)),\n",
    "                     #('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                    ]\n",
    "\n",
    "            union = FeatureUnion(steps)\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('union', union),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=neighbours)),\n",
    "            ])\n",
    "\n",
    "            model = pipeline.fit(X_training, y_training)\n",
    "            y_pred = model.predict(X_dev)\n",
    "\n",
    "            update_stats(\"knn2\", y_dev, y_pred, {'neighbours': neighbours, 'count': count})\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if do_run(\"rfo1\"):\n",
    "    \n",
    "    register_run(\"rfo1\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a random forest setting\",\n",
    "             ['count'])\n",
    "    \n",
    "    for count in tqdm(range(1, 10000, 500), desc=\"# llh counts\"):\n",
    "\n",
    "        steps = [('llh', LLHbasedBinaryVectorizer(count=count)),\n",
    "                 #('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                ]\n",
    "\n",
    "        union = FeatureUnion(steps)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('union', union),\n",
    "            ('rfo', RandomForestClassifier()),\n",
    "        ])\n",
    "\n",
    "        model = pipeline.fit(X_training, y_training)\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(\"rfo1\", y_dev, y_pred, {'count': count})\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp\n",
    "mlp is sensitive to feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "if do_run(\"mlp1\"):\n",
    "    register_run(\"mlp1\",\n",
    "             \"character and word n-grams, with tf-idf feature vectors, used in a multilayer perceptron (adam) setting\",\n",
    "             ['alpha', 'hls', 'min_cn', 'max_cn', 'min_n', 'max_n'])\n",
    "    \n",
    "    for alpha in tqdm(10.0 ** -np.arange(1, 7), desc=\"alpha\"):\n",
    "        for hls in tqdm([(5,2), (5,5), (10,2), (10,5), (50,2), (50,5), (50,10)], desc=\"hls\"):\n",
    "\n",
    "            for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "                for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "\n",
    "                    for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "                        for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "                            \n",
    "                            steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                                     ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                                    ]\n",
    "\n",
    "                            union = FeatureUnion(steps)\n",
    "\n",
    "                            pipeline = Pipeline([\n",
    "                                ('union', union),\n",
    "                                ('mlp1', MLPClassifier(solver='adam', alpha=alpha, hidden_layer_sizes=hls, random_state=1)),\n",
    "                            ])\n",
    "\n",
    "                            model = pipeline.fit(X_training, y_training)\n",
    "                            y_pred = model.predict(X_dev)\n",
    "\n",
    "                            update_stats(\"mlp1\", y_dev, y_pred, {'alpha': alpha, 'hls': hls, 'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if do_run(\"xgb1\"):\n",
    "    register_run(\"xgb1\",\n",
    "             \"character and word n-grams, with tf-idf feature vectors, used in a extreme gradient boost setting\",\n",
    "             [])\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "    parameters = {'nthread':[1], #when use hyperthread, xgboost may become slower\n",
    "                  'objective':['binary:logistic'],\n",
    "                  'learning_rate': [0.05], #so called `eta` value\n",
    "                  'max_depth': [6],\n",
    "                  'min_child_weight': [11],\n",
    "                  'silent': [1],\n",
    "                  'subsample': [0.8],\n",
    "                  'colsample_bytree': [0.7],\n",
    "                  'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "                  'missing':[-999],\n",
    "                  'seed': [1337]}\n",
    "\n",
    "    from sklearn.cross_validation import *\n",
    "    clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                       cv=StratifiedKFold(y_training[use], n_folds=5, shuffle=True), \n",
    "                       scoring='accuracy',\n",
    "                       verbose=2, refit=True)\n",
    "\n",
    "    #bst = clf.fit(X_training, y_training)\n",
    "    #xgb_model = xgb.XGBClassifier(nthread=1, objective='binary:logistic', learning_rate=0.05, max_depth=6, min_child_weight=11, missing=-999, n_estimators=1000, subsample=0.8, colsample_bytree=0.7)\n",
    "\n",
    "    #xgb.plot_importance(xgb_model)\n",
    "    #plt.show()\n",
    "    \n",
    "    #update_stats(\"xgb1\", y_dev, y_pred, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "from abc import ABCMeta\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "\n",
    "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.svm_ = [] # fuggly\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # so we don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "\n",
    "        # Count raw events from data\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
    "                                 dtype=np.float64)\n",
    "        self._compute_ratios(X, Y)\n",
    "\n",
    "        # flugglyness\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
    "            Y_i = Y[:,i]\n",
    "            svm.fit(X_i, Y_i)\n",
    "            self.svm_.append(svm) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_effective_classes = self.class_count_.shape[0]\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        D = np.zeros((n_effective_classes, n_examples))\n",
    "\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            D[i] = self.svm_[i].decision_function(X_i)\n",
    "        \n",
    "        return self.classes_[np.argmax(D, axis=0)]\n",
    "        \n",
    "    def _compute_ratios(self, X, Y):\n",
    "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "\n",
    "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
    "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
    "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
    "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
    "        check_array(self.ratios_)\n",
    "        self.ratios_ = sparse.csr_matrix(self.ratios_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "if do_run(\"nbs1\"):\n",
    "    register_run(\"nbs1\",\n",
    "             \"character and word n-grams, with tf-idf feature vectors, used in a naive bayes/svm setting\",\n",
    "             ['C', 'min_cn', 'max_cn', 'min_n', 'max_n'])\n",
    "    \n",
    "    for C in tqdm([0.01, 0.1, 1.0], desc=\"C\"):\n",
    "\n",
    "        for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "            for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "\n",
    "                for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "                    for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "\n",
    "                        steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                                 ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                                ]\n",
    "\n",
    "                        union = FeatureUnion(steps)\n",
    "\n",
    "                        pipeline = Pipeline([\n",
    "                            ('union', union),\n",
    "                            ('nbs', NBSVM(C=C)),\n",
    "                        ])\n",
    "\n",
    "                        model = pipeline.fit(X_training, y_training)\n",
    "                        y_pred = model.predict(X_dev)\n",
    "\n",
    "                        update_stats(\"nbs1\", y_dev, y_pred, {'C': C, 'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
