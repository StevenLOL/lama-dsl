{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ucto\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can enter the parts that you want to run. The identifiers are the run names, and you can find them further\n",
    "# in the notebook. Use \"all\" to run all of them; this option overrules all other run names. Also note that it takes a\n",
    "# seriously large amount of time to run them all.\n",
    "\n",
    "_run = set([])\n",
    "_use_subset = False\n",
    "_show_graphics = True\n",
    "\n",
    "import sys\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # running in console\n",
    "    _show_graphics = False\n",
    "    from tqdm import tqdm as tqdm, trange as tnrange\n",
    "    \n",
    "    for v in sys.argv:\n",
    "        if v.startswith(\"r:\"):\n",
    "            _run.add(v[2:])\n",
    "        elif v == \"o:subset\":\n",
    "            _use_subset = True\n",
    "else:\n",
    "    # running in notebook\n",
    "    from tqdm import tqdm_notebook as tqdm, tnrange as tnrange\n",
    "    \n",
    "    _run.add(\"all\")\n",
    "    \n",
    "    _use_subset = True\n",
    "    pass\n",
    "\n",
    "def do_run(name):\n",
    "    return name in _run or \"all\" in _run\n",
    "\n",
    "_run_stats = {}\n",
    "\n",
    "def register_run(name, description, params):\n",
    "    if name not in _run_stats:\n",
    "        _run_stats[name] = {}\n",
    "        _run_stats[name]['description'] = description\n",
    "        _run_stats[name]['params'] = {}\n",
    "        for param in params:\n",
    "            _run_stats[name]['params'][param] = ''\n",
    "        \n",
    "        _run_stats[name]['best'] = 0\n",
    "        _run_stats[name]['best_predictions'] = []\n",
    "        _run_stats[name]['orig_predictions'] = []\n",
    "        _run_stats[name]['history'] = []\n",
    "\n",
    "def update_stats(name, y_test, y_pred, params):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    _run_stats[name]['history'].append(accuracy)\n",
    "    if accuracy > _run_stats[name]['best']:\n",
    "        _run_stats[name]['best'] = accuracy\n",
    "        for param in params:\n",
    "            _run_stats[name]['params'][param] = params[param]\n",
    "        _run_stats[name]['best_predictions'] = y_pred\n",
    "        _run_stats[name]['orig_predictions'] = y_test\n",
    "        \n",
    "def summarise_run(name):\n",
    "    if name not in _run_stats:\n",
    "        print(\"Name %s is not registrered.\" % (name))\n",
    "        return\n",
    "    \n",
    "    print(\"Model:    \\n\\t%s\" % (name))\n",
    "    print(\"Settings: \\n\\t%s\" % (_run_stats[name]['params']))\n",
    "    print(\"Accuracy: \\n\\t%s\" % (_run_stats[name]['best']))\n",
    "    \n",
    "    cm = confusion_matrix(_run_stats[name]['orig_predictions'], _run_stats[name]['best_predictions'])\n",
    "    \n",
    "    if _show_graphics:\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm, classes=[\"BEL\", \"DUT\"], title=name + \" \" + str(_run_stats[name]['params']))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(cm)       \n",
    "\n",
    "def summarise_all():\n",
    "    for name in _run_stats.keys():\n",
    "        summarise_run(name)\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(file):\n",
    "    with open(file) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucto_config = \"tokconfig-nld\"\n",
    "tokeniser = ucto.Tokenizer(ucto_config, sentenceperlineinput=True, sentencedetection=False, paragraphdetection=False)\n",
    "\n",
    "# We read the file with ucto and tokenise it according to its default Dutch tokenisation scheme, which is rule-based\n",
    "# and definitely better than a plain whitespace tokeniser from sklearn. Afterwards we concatenate the tokens back to a \n",
    "# whitespace seperated line, which can then be normally processed with sklearn's tokenisers.\n",
    "def read_data(file):\n",
    "    text = {}\n",
    "    with open(file) as f:\n",
    "        for line in tqdm(f):\n",
    "            sentence, language = line.strip().split(\"\\t\")\n",
    "            tokeniser.process(sentence)\n",
    "\n",
    "            if language not in text:\n",
    "                text[language] = []\n",
    "\n",
    "            current_line = []\n",
    "            for token in tokeniser:\n",
    "                current_line.append(str(token))\n",
    "                if token.isendofsentence():\n",
    "                    #print(current_line)\n",
    "                    text[language].append(\" \".join(current_line))\n",
    "                    current_line = []\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading development set from pickle.\n",
      "development set\n",
      "\t LAN\t size \t avg length\n",
      "\t BEL \t 250 \t 40.456\n",
      "\t DUT \t 250 \t 40.088\n",
      "Done reading training set from pickle.\n",
      "training set\n",
      "\t LAN\t size \t avg length\n",
      "\t BEL \t 150000 \t 40.273626666666665\n",
      "\t DUT \t 150000 \t 40.37152\n"
     ]
    }
   ],
   "source": [
    "# If this is the first run, then we have to tokenise the text. In other cases we probably have saved a pickled version\n",
    "# somewhere. If not, we will tokenise the text anyway. No worries.\n",
    "\n",
    "# First the development set\n",
    "try:\n",
    "    with open('data/dev.txt.pickle', 'rb') as f:\n",
    "        _l_dev_text = pickle.load(f)\n",
    "        print(\"Done reading development set from pickle.\")\n",
    "except IOError:\n",
    "    _l_dev_text = read_data('data/dev.txt')\n",
    "    print(\"Done tokenising development set.\")\n",
    "    with open('data/dev.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_dev_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing development set from pickle.\")\n",
    "\n",
    "print(\"development set\")\n",
    "print(\"\\t LAN\\t size \\t avg length\")\n",
    "for l in _l_dev_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_dev_text[l]), \"\\t\", sum([len(x.split()) for x in _l_dev_text[l]])/len(_l_dev_text[l]))\n",
    "\n",
    "# And then the training set. This takes bit more time...\n",
    "try:\n",
    "    with open('data/train.txt.pickle', 'rb') as f:\n",
    "        _l_trn_text = pickle.load(f)\n",
    "        print(\"Done reading training set from pickle.\")\n",
    "except IOError:\n",
    "    _l_trn_text = read_data('data/train.txt')\n",
    "    print(\"Done tokenising training set.\")\n",
    "    with open('data/train.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_trn_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing training set from pickle.\")\n",
    "\n",
    "print(\"training set\")\n",
    "print(\"\\t LAN\\t size \\t avg length\")\n",
    "for l in _l_trn_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_trn_text[l]), \"\\t\", sum([len(x.split()) for x in _l_trn_text[l]])/len(_l_trn_text[l]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert the training and development material into the right shape, and make sure that we also keep track of\n",
    "# the labels.\n",
    "\n",
    "X_training = []\n",
    "y_training = []\n",
    "for l in _l_trn_text.keys():\n",
    "    for s in _l_trn_text[l]:\n",
    "        X_training.append(s)\n",
    "        y_training.append(l)\n",
    "X_training = np.array(X_training)\n",
    "y_training = np.array(y_training)\n",
    "\n",
    "\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "for l in _l_dev_text.keys():\n",
    "    for s in _l_dev_text[l]:\n",
    "        X_dev.append(s)\n",
    "        y_dev.append(l)\n",
    "X_dev = np.array(X_dev)\n",
    "y_dev = np.array(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training subset\n"
     ]
    }
   ],
   "source": [
    "# Sometimes for testing whether some code words, you might want to use a subset. Use this one. Or another one. I don't\n",
    "# care. \n",
    "\n",
    "import random\n",
    "#use = random.sample(range(1, 299999), 100000)\n",
    "use = random.sample(range(1, 299999), 10000)\n",
    "\n",
    "if _use_subset:   \n",
    "    X_training = X_training[use]\n",
    "    y_training = y_training[use]\n",
    "    \n",
    "    print(\"training subset\")\n",
    "    #print(\"\\t LAN\\t size \\t avg length\")\n",
    "    #for l in _l_trn_text.keys():\n",
    "    #    print(\"\\t\", l, \"\\t\", len(_l_trn_text[l]), \"\\t\", sum([len(x.split()) for x in _l_trn_text[l]])/len(_l_trn_text[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerFeatures():\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, features, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6):\n",
    "        self.run_name = name\n",
    "        self.run_description = description\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        self.features = features\n",
    "        self.feature_selection = feature_selection\n",
    "        self.scaler = scaler\n",
    "    \n",
    "        self.run_min_cn = min_cn\n",
    "        self.run_max_cn = max_cn\n",
    "        \n",
    "        self.run_min_n = min_n\n",
    "        self.run_max_n = max_n\n",
    "    \n",
    "        register_run(self.run_name, self.run_description, [])\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        #print(\"Classifier:\", self.classifier)\n",
    "        #print(\"Feature extraction:\", self.features)\n",
    "        #print(\"Feature selection:\", self.feature_selection)\n",
    "        #print(\"Feature scaler:\", self.scaler)\n",
    "                \n",
    "        self.pipeline = Pipeline([\n",
    "            ('features',   FeatureUnion(self.features)),\n",
    "            ('scaler',     self.scaler),\n",
    "            ('selection',  self.feature_selection),\n",
    "            ('classifier', self.classifier),\n",
    "        ])\n",
    "\n",
    "        model = self.pipeline.fit(X_training, y_training)\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(self.run_name, y_dev, y_pred, {'min_cn': self.run_min_cn, 'max_cn': self.run_max_cn, 'min_n': self.run_min_n, 'max_n': self.run_max_n})\n",
    "        \n",
    "        return accuracy_score(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "class CountVectorFeatures(VectorizerFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6):\n",
    "        \n",
    "        features = [('char', CountVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(CountVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, features, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "        \n",
    "class SelectCountVectorFeatures(CountVectorFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6, k = 100):\n",
    "        features = [('char', CountVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(SelectCountVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "        self.k = k\n",
    "        self.feature_selection = SelectKBest(chi2, k=k)\n",
    "\n",
    "##\n",
    "        \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class TfidfVectorFeatures(VectorizerFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6):\n",
    "        \n",
    "        features = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(CountVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, features, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "        \n",
    "class SelectTfidfVectorFeatures(CountVectorFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6, k = 100):\n",
    "        features = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(SelectCountVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "        self.k = k\n",
    "        self.feature_selection = SelectKBest(chi2, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8a1cee15382f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                                             \u001b[0;34m\"c&w n-grams, counts, random forests\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                             \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                             k=k).run())\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msummarise_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rfo_sc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-38986e688c53>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m         ])\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             delayed(_fit_transform_one)(trans, weight, X, y,\n\u001b[1;32m    738\u001b[0m                                         **fit_params)\n\u001b[0;32m--> 739\u001b[0;31m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0mn_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/lamachine/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mreordered\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodifies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \"\"\"\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0msorted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0mmap_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "if do_run(\"rfo_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"rfo_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, counts, random forests\", \n",
    "                                            RandomForestClassifier(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"rfo_sc\")\n",
    "\n",
    "if do_run(\"mnb_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"mnb_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, counts, mult naive bayes\", \n",
    "                                            MultinomialNB(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"mnb_sc\") \n",
    "\n",
    "if do_run(\"knn_sc\"):\n",
    "    for n in range(1,7):\n",
    "        for k in [100, 1000, 10000, 100000]:\n",
    "             print(n, k, SelectCountVectorFeatures(\"knn_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, counts, knn\", \n",
    "                                                   KNeighborsClassifier(n_neighbors=n), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"knn_sc\") \n",
    "\n",
    "if do_run(\"sgd_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"sgd_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, counts, sgd\", \n",
    "                                               SGDClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"sgd_sc\")\n",
    "\n",
    "if do_run(\"xgb_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"xgb_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, counts, xgboost\", \n",
    "                                               xgb.XGBClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"xgb_sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.57\n",
      "1000 0.544\n",
      "10000 0.568\n",
      "100000 0.586\n",
      "Model:    \n",
      "\txgb_sc\n",
      "Settings: \n",
      "\t{'min_cn': 1, 'max_cn': 8, 'min_n': 1, 'max_n': 6}\n",
      "Accuracy: \n",
      "\t0.586\n",
      "Confusion matrix, without normalization\n",
      "[[157  93]\n",
      " [114 136]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAEmCAYAAADFmJOIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xu8XfOd//HX+5zITUIQdyFKaFFSUkrRzIxxG0o71TJal6qg6Ch+M0U7dFBUq51STJSmLnXpDK1xrZpRlyYlSNRdXBONXAmRmySf3x/re2LlOOfsfU723mvvfd7PPNYje3+/a63vd6+19+d813d911qKCMzMrLZaiq6AmVlv5OBrZlYAB18zswI4+JqZFcDB18ysAA6+ZmYFqFrwlTRcUkjqU60yyqjDBpIeljRP0lcqtM75kj5WiXWZSdpD0gtF18NWjaTJkqZL+na5yzR7y/cQYDVg3Yi4uS1RUo8HN0fEoIh4pRKV6y5Jr0kaXua8X5b0J0kLJD3QzXLOkXRO92tYXyQ9IGl0mfP2k3SlpBmS5kr6H0kbl7ns6O5u4zYR8VBEbN2TZcslaZyko8qcdztJ90qa3d3fyapshyL04PMdKuk5Se9LelnSHm15EbEDcCxwdrnra/bguzbwUkQsKboiBZgL/BS4sOiKNIh/BnYFtgc2At4GLi20RsX4ALgFOKboitQTSX8PXAQcDQwG9gTaN8KeBtaU1FrOOrsMvpK2SK2AHdP7jSTNamtNSNpc0oOS3pP0B0k/l3R9u9V8XdJfU5P89DI+5M6SJkp6N7VCLsnl7Z5ac+9ImlrGX/M+wPIS5T0g6by03vmpxbOOpBtSHR7LtzZTV8qW6fW49JnvTNvgz5K2KOMzbivpvrRtZ0g6M6WfI+kWSdem9T0jaVSp9XUkIv4QEbcAf+3J8rm6jpY0TdK/SJqZ9uPBkvaX9GL6DGfm5t9Z0vi0j6ZLukxS35S3W2pRDUvvd5D0tqSPl6jDMEm3pu/eHEmXpfSjUrfSj9J6XpW0Xw8/6ubAvRExIyIWATcD2/ZkRek78k1JL6X9eG76Lf0pfaduyW2T0ZKm5ZZ9TdLpkp5S1l12s6T+Jcpr20en5fbR0T2pe0S8EBFXA8/0ZPl29erOdlhL0h1pH7+dXm+S8tZOn+/A9H6QpCmSjihRfo9+n534PvDvETEhIpZHxJsR8Wa7edpiTXldrRHR5UTWlH4WGAjcC/wolzce+BHQF9gdeBe4PuUNBwK4EVgd+CQwC9irRHnjga+l14OAz6TXmwHvAYeRdSWsA4zsYj0Dgf8FzitR3gPAFGALYM30WV8E9kob8Vrgl7n5A9gyvR4HzAF2TvPeANxUorzBwHTgNKB/er9LyjsHWATsD7QCFwATOlnPPwFPlbH/vgE8UGq+LpYfDSwF/i1t92PTfvx1qvu2wEJg8zT/TsBn0vYYDjwHnJJb3/lpvwwA/gKcVKL8VmAy8JP0PeoP7J7yjiJrqR2b5juB7I+NOljP7sA7XZQzCniErNU7MH2+n/ZwmwXwO2CNtH0WA/cDH8t9x47Mbd9puWVfAx5N9Vg7bb/jy9xH/5720f7AAmCtDubdFHgH2LTEOrcEoqffmx5sh3WAf0zbfjDwG+C3uXXtDbwFrAdcBfxXGeWPo8zfJ/Ad4I4uvoNL0jxTgGnAZcCAdvMNSL+FA8raPmVuxNvTD+UpoF9uJy4FBubmu56PBt+P5/J/CFxdoqwHyf7KDG2XfgZwW5n1/TywLG2oj3wB2837AHBW7v2Pgbtz7w8EJrX7QuWD7y9yefsDz5co7zDgyU7yzgH+kHu/DbBwFX8AlQi+C4HW9H5w2ga75OZ5HDi4k+VPye83suDwePo+3UMHgbLd8ruSBfs+HeQdBUzJvR+Y6rZBDz7nmsBNafmlwJPA2j3cZgF8tt32+dd237Gf5rZv++D71dz7HwJXlrmP+uTSZpIaLj38DJUKvmVthw6WHQm83S7t0vS9eRNYp4zyu/377GQ9G6XPMhHYEBhK9of6/A7mPZmsBTyp1HrL7fO9CtgOuDQiFqe0jYC5EbEgN9/UDpbNp72eluvKMcBWwPPpkP+AlD4MeLmcykbE7WSthjmU13c1I/d6YQfvB3Wx7Fu51wtKzAulP0f79fVXgSNGkjkRsSy9Xpj+73AbSdoqHTK+Jeld4AdkX1YAIuIDsh/FdsCPI31juzAMeD0ilnaSv2J75b6LpfZBR34O9CNrga0O3Arc3YP1tKnldwqyfZTfRuUuV21lbQdJAyX9p6TX0/fmQWCIVu4/HUv2vRkXEXPKLL8n27K9tu/8pRExPSJmA5eQBfMV0u/0+8DXgE+VWmnJ4CtpENmJm6uBcyStnbKmA2tLGpibfVgHq8inbUqJPsiIeCkiDiM7vLgI+C9Jq5MF8bL7ayJiHtnh7TblLlMjU8kOu5rVFcDzwIiIWAM4E1BbprIRBGcDvwR+LKlfifVNBTatwR+gkWQ/6rmpgXEpsLOkoSWWs8o4Ddia7IhqDbITWpC+OykIjyXrBvxm23mXWoiIt8m6GvINhY4aDesDa5F1l5QcSVFOy/c/gIkR8Q3gTuDKVKHXyZrh50jqK2lXskP09r6X/qptS3am8OYO5llB0lclrRsRy8n6piBrxt8A7KVsCFUfZSfFRpao+2Ky/uh6cgewoaRTlA1vGixpl0oXIqk1najpA7RI6i9ptVz+aypz+FE3DSbr+5+fTqSdkCtTZK3eq8mOSKYD55ZY36NpvgslrZ4+x2erUO/HgCMkrZm20zeBv6ZWTtvJm3FVKLeuKNOf9LtJ27tfLr9a22EwWQvzndTAaz9k60yygPd14GLgWpU5qqBCfgmcLGk9SWsB3yb7Lee1/b4WU4ZSox0OAvblwx/QqcCOkg5P7w8n65ObA5xHFljbF/xHsr7X+8lO1v2+RJ32BZ6RNJ8s8B8aEQsj4g2yZv5pZMOoJgE7lFjX8lKfsdYi4j3g78n+UL0FvAT8TXfXI+lwSV2dkf4a2Zf5CmCP9PqqtGxfssPrCd0ttwynk50MfC+Vl/9j+y2yI5rvpZbB0cDRyo2XbC91dxxI1gf5BlkLpNsXzCi7mGF+iXovItsfs8i+a1/I5Q8j6+draJI2VTaqZ9NOZtmM7LvS9t1aCOQvAqnWdvgp2Qmr2WTfy3vaMiTtRBZ7jkjfh4vIAvF3KlW4pDMlddXNdC7ZH+gXyU6CPkl28jiv7Y9BlyOsVpRZRuu4bJJuJuvQLnugcTVJGgOcCOwWEe8XXZ96IWl34MTUvWMlpD9Wk4HtU591r+Tt0DVJXwYui4j1ypp/VYKvpE+TtUJfJRsK8ltg14h4sscrraB0+HIt2VCTkyIb92pmVlGSHicbMXN+RPyyrGVWMfgeCFxOdgg7DbignIJT876jQ80fRMQPelyhOpEOozs8hImIejgDXVfSIfCznWRvk7qcejVlF7Kc2UHWQxHR0wtLGl7qetusg6zjIuKGWtenOyra7WBmZuWpq5NRZma9RdGD9+uC+gwI9R1cdDWsBz71ic5O2lu9e+KJx2dHxLqVWl/rGptFLF1YekYgFs66NyL2rVTZPeHgC6jvYPpt/eWiq2E98MifLyu6CtZDA1bT65VcXyxdWPbveNGknxd+8YyDr5k1CYEapyfVwdfMmoOAllpe9LZqHHzNrHlIpeepEw6+ZtYk3O1gZlYMt3zNzGpMuOVrZlZ7csvXzKwQbvmamdWaPNTMzKzmhLsdzMwK4W4HM7Na8zhfM7NitLjbwcystjzO18ysCB7tYGZWDI92MDMrgLsdzMxqTL682MysGA3U8m2cmpqZldLW+i01lVyNrpE0U9LTubRzJL0paVKa9s/lnSFpiqQXJO1TTlUdfM2sSaSLLMqZShsHdPR0459ExMg03QUgaRvgUGDbtMzlkkoOu3DwNbPm0PYMt3KmEiLiQWBumSUfBNwUEYsj4lVgCrBzqYUcfM2sSVS05duZkyQ9lbol1kppGwNTc/NMS2ldcvA1s+ZRfp/vUEkTc9OYMtZ+BbAFMBKYDvx4Varq0Q5m1jzKb9XOjohR3Vl1RMxYUYx0FXBHevsmMCw36yYprUtu+ZpZ86jQaIeOV60Nc2+/ALSNhLgdOFRSP0mbAyOAR0utzy1fM2sOqtwtJSXdCIwm656YBpwNjJY0EgjgNeA4gIh4RtItwLPAUuDEiFhWqgwHXzNrGmqpTPCNiMM6SL66i/nPB87vThkOvmbWFLKnCPnyYjOz2lKaGoSDr5k1Cbnla2ZWBAdfM7MCOPiamdWaQH6ApplZbcl9vmZmxXDwNTMrgIOvmVkBHHzNzGrNF1mYmRXDLV8zsxoToqVCN9apBQdfM2sejdPwdfA1syYhdzuYmRXCwdfMrAAOvmZmNebLi83MiuAb61gtXXn24ey353bMmvseow75AQBnHbc/X//ibsx6ez4AZ192O/c+/CyH7jeKU47ca8WynxyxEbsedhFPvVjyKddWA62C1jRSatlyWBbQpwXa4kkEfLC8uPo1Ard8rWau+58JXHnzH/nFuUeslH7p9f/HT6+7f6W0m+6eyE13TwRg2y034pZLjnXgrRMiC7xL0jNvV2uB5QFLc8G2VVkwXuoA3KlGCr6NMyLZOvTIEy8zd96Cbi/35X134jf3PlGFGllPSFmwbbM8PmzxWjeozKkOOPg2qeMP3ZNHbz6DK88+nCGDB3wk/0t778gt90wsoGbWkWgXbFtbsoAMWWu3X2uW5lZv1ySVNdWDhgi+kpZJmiRpsqQnJO2W0odLWpjy2qYjUt5rkoYWW/NiXPWbh9jmwHPY5dALeWv2u1x46hdXyv/0dpuxYNEHPPvy9IJqaO0FWT9v39ZsyreCly6Hxcuy/D4N8YstRrmBt16Cb6P0+S6MiJEAkvYBLgA+l/JebsuzzMy57614fc2tj3Drz45fKf+QfXZyq7cOLQtYlvp8+7RkreH2+X0dfLtUL4G1HI24K9cA3i66EvVsg6FrrHh90N/usFILVxL/uPeO/Obex4uompWpRVmwzYeSVn00INvK1KKypnrQKC3fAZImAf2BDYG/zeVtkfLanBwRD5VaoaQxwBgAVhtUwarW1q8uOIo9dhrB0CGDmHLPuZx75V3sudMItt96EyKC16fP5eTzblwx/+47bsm0t97mtTfnFFhr60jf1g9ft/Xt9sn1/XqoWWmVavlKugY4AJgZEdu1yzsN+BGwbkTMVlbofwD7AwuAoyKi5NnsRgm++W6HXYFrJbVtkB51O0TEWGAsQMvA9Rq2PXHkGeM+kvar347vdP6HHn+Jzx354yrWyHqqbZhZnoNtN1T2xjrjgMuAa1cqQhoG7A28kUveDxiRpl2AK9L/XWq4boeIGA8MBdYtui5mVj9EdpRQzlRKRDwIzO0g6yfAv5CdI21zEHBtZCYAQyRtWKqMhgu+kj4OtAI+bjaznG6NdhgqaWJuGlNy7dJBwJsRMbld1sbA1Nz7aSmtS43S7TAg168r4MiIWJY2Yvs+32si4mfp9VOS2g7cbomIU2tUXzMrQDd6HWZHxKjy16uBwJlkXQ4V0RDBNyJaO0l/DfjoFQRZ3vAqVsnM6o2gpXojGbYANgcmp0bfJsATknYG3gSG5ebdJKV1qSGCr5lZKaJ6wTci/gKst6Is6TVgVBrtcDtwkqSbyE60zYuIklcwNVyfr5lZZyp1wk3SjcB4YGtJ0yQd08XsdwGvAFOAq4BvllNXt3zNrGlUaqhZRBxWIn947nUAJ3a3DAdfM2sOZbZq64WDr5k1hWycb+NEXwdfM2sSquZoh4pz8DWzpuGWr5lZrbnP18ys9tzna2ZWkAaKvQ6+ZtY83PI1MytAA8VeB18zaw6q7o11Ks7B18yaRP08mbgcDr5m1jQaKPY6+JpZ83DL18ys1nyRhZlZ7fkiCzOzgni0g5lZAdzyNTOrNff5mpnVnjzO18ysGA0Uex18zax5tDRQ9HXwNbOm0UCx18HXzJqDBK0eagaS1ugqPyLerVbZZtY7+YRb5hkgyC48adP2PoBNq1i2mfVCDRR7qxd8I2JYtdZtZtaeyIabNYqWWhQi6VBJZ6bXm0jaqRblmlnv0qLyplIkXSNppqSnc2nnSnpK0iRJv5e0UUqXpJ9JmpLydyyrrj39kOWSdBnwN8DXUtIC4Mpql2tmvYyyiyzKmcowDti3XdrFEbF9RIwE7gD+LaXvB4xI0xjginIKqMVoh90iYkdJTwJExFxJfWtQrpn1IqJyox0i4kFJw9ul5QcJrE527grgIODaiAhggqQhkjaMiOldlVGL4PuBpBZSRSWtAyyvQblm1st044TbUEkTc+/HRsTY0uvX+cARwDyyI3qAjYGpudmmpbQug28t+nx/Dvw3sK6k7wMPAxfVoFwz62W60e0wOyJG5aaSgRcgIs5KgwluAE5albpWveUbEddKehzYKyUdEhFPd7WMmVl3qbZ3NbsBuAs4G3gTyI/u2iSldakmox2AVuADYEkNyzSzXqZFKmvqCUkjcm8PAp5Pr28HjkijHj4DzCvV3ws1aPlKOgv4J+A2sj7xX0u6ISIuqHbZZta7VKrhK+lGYDRZ3/A0shbu/pK2Jjtn9TpwfJr9LmB/YArZaK6jyymjFifcjgA+FRELYEWH9ZOAg6+ZVUyFRzsc1kHy1Z3MG8CJ3S2jFsF3erty+lDiLKCZWbeVP4a3LlTzxjo/IRteNhd4RtK96f3ewGPVKtfMeq8Gir1Vbfm2jWh4Brgzlz6himWaWS/mli8QER32j5iZVYMo774N9aIWox22AM4HtgH6t6VHxFbVLtvMepdGavnWYsztOOCXZH+Y9gNuAW6uQblm1suozKke1CL4DoyIewEi4uWI+C5ZEDYzq5i2xwiVM9WDWgw1W5xurPOypOPJLrsbXINyzayXaaRuh1oE32+T3X7tW2R9v2sCX69BuWbWyzRQ7K3JjXX+nF6+x4c3VDczqyjR8/s2FKGaF1ncxoc3G/6IiPhitco2s16otnc1W2XVbPleVsV1V9Tmm2/ID8edVXQ1rAf2uOiBoqtgdcR9vkBE3F+tdZuZtSeg1cHXzKz26mQUWVkcfM2saTj4dkBSv4hYXKvyzKx3yR4j1DjRt+pXuEnaWdJfgJfS+x0kXVrtcs2s92lReVM9qMXlxT8DDgDmAETEZD585LKZWcW0PUSz1FQPatHt0BIRr7c7HFhWg3LNrBfJbilZJ5G1DLUIvlMl7QyEpFbgZODFGpRrZr1Ma+PE3poE3xPIuh42BWYAf0hpZmYVo1V4LHwRanFvh5nAodUux8ysgWJvTZ5kcRUd3OMhIsZUu2wz613qZSRDOWrR7fCH3Ov+wBeAqTUo18x6EZ9wayciVnpkkKTrgIerXa6Z9T4NFHtrMs63vc2B9Qso18yambIb65QzlVyVdI2kmZKezqVdLOl5SU9Juk3SkFzeGZKmSHpB0j7lVLcWV7i9LWlumt4B7gPOqHa5Zta7tD06vkJXuI0D9m2Xdh+wXURsTzZc9gwASduQDSrYNi1zeRpW26Wqdjsou7JiB7LntgEsj4hOb7BuZrYqKnXCLSIelDS8Xdrvc28nAF9Krw8Cbkr3rnlV0hRgZ2B8l3WtTFU7lgLtXRGxLE0OvGZWNZLKmoChkibmpu6Ovvo6cHd6vTErDyKYltK6VIvRDpMkfSoinqxBWWbWS7V1O5RpdkSM6lE50lnAUuCGnizfpprPcOsTEUuBTwGPSXoZeJ9sG0VE7Fitss2sF6rBTXMkHUV2o7C/yx3JvwkMy822CR92tXaqmi3fR4Edgc9XsQwzMyBr1fWp4lUWkvYF/gX4XEQsyGXdDvxa0iXARsAIsvjXpWoGXwFExMtVLMPMbIVKtXwl3QiMJusbngacTTa6oR9wX+o3nhARx0fEM5JuAZ4l6444MSJK3rmxmsF3XUmndpYZEZdUsWwz63VEC5WJvhFxWAfJV3cx//nA+d0po5rBtxUYBBXaGmZmXRCNdYVbNYPv9Ij49yqu38zsQ3X0iKByVL3P18ysVnxjnczfVXHdZmYrcbdDEhFzq7VuM7OOtDZQv0MtrnAzM6s6UcxtGnvKwdfMmoNou29DQ3DwNbOm0Tih18HXzJqEHyNkZlaQxgm9Dr5m1jREi0c7mJnVlkc7mJkVxKMdzMwK0Dih18HXzJqFx/mamdWe+3zNzArilq+ZWQEaaKSZg6+ZNYes26Fxoq+Dr5k1jQbqdXDwNbNmIeSWr5lZ7bnla2ZWY+7zNTMrgqClgQb6OviaWdNwn6/VzE7D1mSDwf1YvHQ5f3hxNgAbr9mfbTYYxOB+ffjfl+bwzsIPVlpmwGot7L31ujw7Yz4vzXq/iGob8L0Dtmb3Ldfh7fc/4NCrHgPg+M8NZ88RQwlg7vtL+P7/PM/s+UsA2HHTIZy295b0aRHvLPiA466fVGDt6092M/Wia1G+BmqkW0den7uQR15d+UHR7y5ayvjX3mb2+0s6XGb7jdbgrfcW16J61oU7Jr/Ft256aqW068ZP5Z9+MZHDfzGRh1+awzf2GA7AoH59+Nd9R3DqLX/hK2Mf4zu3PlNAjeufyvxXcj3SNZJmSno6l3aIpGckLZc0qt38Z0iaIukFSfuUU1cH3wY3+/0lLFkaK6W9t3gp8xcv63D+jdbox4Ily3h30dJaVM+68OTUeby7cOX98P6SD/fbgL6tRGT7dt/t1uP/XpjNjHezP5pvL1j5aMYyUnlTGcYB+7ZLexr4IvDgymVqG+BQYNu0zOWSWksV4G6HXqS1RWy13iAeemUuW627etHVsU6cMHpz/uGT6zN/0TKOvyHrWth07YH0aRFXfnUkA/u2ctNj07jrLzMKrmn9qVSfb0Q8KGl4u7TnoMP7RxwE3BQRi4FXJU0BdgbGd1VG3bZ8JS2TNCk18ydLOk1SS8o7StJl7eZ/QNIoSX9Oy70haVZ6Pan9huyNtll/EC/Nep9ly6P0zFaYKx54lQMuncA9z8zgy6M2BrI/nB/fcDCn3PwUJ9/4FMfsvhmbrj2g4JrWFyFaVd4EDJU0MTeNWYWiNwam5t5PS2ldqueW78KIGAkgaT3g18AawNldLRQRu6RljgJGRcRJVa5nw1h7YF82HtKfT240mNVaWyBg+fLg5TkLiq6adeDup2fwH1/ZnrEPvsbMdxczb+EHLPpgOYs+WM6Tb8xjxHqDeGPuwqKrWT/K71IAmB0Ro0rPVj112/LNi4iZwBjgJDXSPePqzB9fnsM9z83inudmMWXW+zw/c74Db50ZttaHrdnPbTWU19L++eOLsxm5yZq0SvTr08J2G62xIs8+pDKnCnsTGJZ7v0lK61I9t3xXEhGvpE7s9SqxvnSYMQZg6IYljxDq1s6bDmHooL7069PCfp9Yj+dmvMeSpcvZYeM16denhc9uvhbzFi3l4Vfmll6Z1dR5B3+CnTYbwpABq3HHybsy9sFX+eyW67DZ2gNZHsFb7y7igrtfBOC1OQv40ytz+fWxo4iA302azsseJriSbKhZIW2z24FfS7oE2AgYATxaaqGGCb7tdNZpWXZnZkSMBcYCbLHtDg3bCfroG+90mP7Xd2d2udxzM+ZXozrWDd/97XMfSbt98ludzn/9hKlcP2Fqp/lWuVatpBuB0WR9w9PIujvnApcC6wJ3SpoUEftExDOSbgGeBZYCJ0ZEx8ONchom+Er6GLAMmAnMAdZqN8vawOxa18vM6kiFom9EHNZJ1m2dzH8+cH53ymiIPl9J6wJXApdFNvDxMeCzkjZI+aOAfqx8xtHMeplKXWRRC/Xc8h0gaRKwGllT/jrgEoCImCHpn4G70vCz+cBhEbG8sNqaWeEa6fLiug2+EdHlFSIR8Tvgd13kjyO7SsXMegsHXzOz2sqGkTVO9HXwNbPm0L2LLArn4GtmTaOBYq+Dr5k1kQaKvg6+ZtYkVNQVbj3i4GtmTaFK922oGgdfM2seDRR9HXzNrGl4qJmZWQEaqMvXwdfMmkcDxV4HXzNrEg12xs3B18yaQoE3U+8RB18zaxqNE3odfM2smTRQ9HXwNbOm4aFmZmYFaKAuXwdfM2seDRR7HXzNrDkIUAM1fR18zaw5+GbqZmbFaKDY6+BrZk2kgaKvg6+ZNQk11FCzlqIrYGZWKVJ5U+n16BpJMyU9nUtbW9J9kl5K/6+V0iXpZ5KmSHpK0o7l1NXB18yaQjbaoTLBFxgH7Nsu7TvA/RExArg/vQfYDxiRpjHAFeUU4OBrZk1DZf4rJSIeBOa2Sz4I+FV6/Svg4Fz6tZGZAAyRtGGpMhx8zaxpVLDl25H1I2J6ev0WsH56vTEwNTfftJTWJZ9wM7Om0Y24OlTSxNz7sRExttyFIyIkRTeq9hEOvmbWHLrXqp0dEaO6WcIMSRtGxPTUrTAzpb8JDMvNt0lK65K7HcysiajMqUduB45Mr48EfpdLPyKNevgMMC/XPdEpt3zNrCm0jXaoyLqkG4HRZN0T04CzgQuBWyQdA7wOfDnNfhewPzAFWAAcXU4ZDr5m1jRaKhR8I+KwTrL+roN5Azixu2U4+JpZ02ikK9wcfM2seTRO7HXwNbPm0UCx18HXzJrDKl5AUXMOvmbWNNzna2ZWALd8zcwK4OBrZlZzjXUzdQdfM2sKlbzCrRZ8bwczswK45WtmTaORWr4OvmbWNNzna2ZWY1LlbqxTCw6+ZtY8HHzNzGrP3Q5mZgXwCTczswI0UOx18DWzJtJA0dfB18yagoCWBup3UPb4od5N0iyyB+I1q6HA7KIrYd3W7Ptts4hYt1Irk3QP2TYrx+yI2LdSZfeEg28vIGliRIwquh7WPd5vzc33djAzK4CDr5lZARx8e4exRVfAesT7rYm5z9fMrABu+ZqZFcDB18ysAA6+Zg1A0i6ShhVdD6scB1+zOidpH+BGyr+AwBqAg2+Tc2upsaXA+0vghIh4UpJvCdAkHHybmKTVgQcknVJ0Xaz7UuC9DJgIfELSGhGxVJJ/t03AO7FJSfoEsAQ4FDhO0gm5vJbc69YCqmclSNqeLPB+A/g3YDhwqqRBEbHcAbjxeQc2IUn7kx2qfiwiHgO+RvbD/SZARCxP8x0KnCQ10K2gegF/lwkpAAAGM0lEQVRJOwD9gS9ExB+BZ4A7gTWB0x2Am4N3XpNJh6rfA86OiBckDYmIicBXyAVgSd8ALgZ+H77Spt7sA1wEDJHUNyI+AO7nwwD8bUmD2/6IWmPyFW5NJB2qTgL2ioj/lbQF8J/A6RExSdIo4HrgeWBb4EsRMbm4GltnJH0L2A84H/hzRHyQWrqjybqSXgZ+6D+cjcvBt4lIGgRcC8wELgSuBu6JiIsltaRD1ZHAz8nOnj9VYHUtJ/XRKyKezaV9G/h74NyIGJ/SWoA9gOcjYkYhlbWKcPBtApKGAssjYq6kvsA1wGHAKRFxaS7w7knWfzgvIpYWWWf7kKTBwKnAMOBHEfF8Lu9U4GDgixHRzDdW73Xc59vg0sm1u4ArJZ0fEUuA44GbgN0gO8Em6Wiy1nB/B976EhHvAf8FTCU7AbpNLu8S4EXgWwVVz6rEwbeBSdoXOJOsX/AHwKaSBkbEfOBoYKmk6yR9FTgGGBMRbxZXY8uTNELSbpI+B7wGXEn22KAT8gEYeBaYU0AVrYrc7dCgJK1N9kP9x4i4TdLOwO+A24DWiDgudUH8N/A3wM75/kQrlqR/AM4le3bgYGAEsD+wCDgc2BK4DlgbOB04IiKeK6a2Vg0Ovg0s/YDPA44CfgT8CfgF2SHsqxFxaLrKbc2I+GthFbWVpCOWc4B/TeN4kXQO2X7cD3iF7MjlQGAecEFE/KWIulr1OPg2uPRDvgs4MyIuTGmDyFrBX/FJmvqSO2L5fETcIal/RCxKed8na/XuFBHz0pFLpHG+1mTc59vgIuIeskH5R0sakpIPAQYAiwurmHUoIuaStWgvkLRORCyS1C/lnU120m1Eer/Egbd5+Q5JTSAi7ks3z3lY0uVkg/DHpLPoVmci4k5Jy4FHJY2KiLclrZYC7btk/b7W5Bx8m0RE3J1uknMr8KmIeKboOlnn0v46CZiYC8BHABuQXSRjTc59vk0mDTVbUHQ9rDyS9gN+CFxOdgOkMRHxdLG1slpw8DUrmKQD8BFLr+Pga1YHfMTS+zj4mpkVwEPNzMwK4OBrZlYAB18zswI4+JqZFcDB17pF0jJJkyQ9Lek3kgauwrpGS7ojvf68pO90Me+QtufPdbOMcySdXm56u3nGSfpSN8oaLsljdK0sDr7WXQsjYmREbEf2aPrj85nKdPt7FRG3t90YqBNDgG4HX7N65eBrq+IhYMvU4ntB0rXA08AwSXtLGi/pidRCHgTZXdgkPS/pCeCLbSuSdJSky9Lr9SXdJmlymnYjewrHFqnVfXGa7/9JekzSU+mOYG3rOkvSi5IeBrYu9SEkHZvWM1nSf7drze8laWJa3wFp/lZJF+fKPm5VN6T1Pg6+1iOS+pDde7btPrMjgMsjYlvgfeC7ZE9R3hGYSPbY+v7AVWR39dqJ7D4GHfkZ8MeI2AHYkey5c98BXk6t7v8nae9U5s7ASGAnSXtK2onsxkIjyW5O/ukyPs6tEfHpVN5zZE/9aDM8lfEPZI9q6p/y50XEp9P6j5W0eRnlmK3gG+tYdw2QNCm9fojsCckbAa9HxISU/hlgG+ARSQB9gfHAx8lu8v4SgKTrgTEdlPG3wBEAEbEMmCdprXbz7J2mJ9P7QWTBeDBwW9vVYpJuL+MzbSfpPLKujUHAvbm8WyJiOfCSpFfSZ9gb2D7XH7xmKvvFMsoyAxx8rfsWRsTIfEIKsO/nk4D7IuKwdvOttNwqEtkTHv6zXRmn9GBd44CDI2KypKOA0bm89peARir75IjIB2kkDe9B2dZLudvBqmEC8FlJWwJIWl3SVsDzwHBJW6T5Dutk+fuBE9KyrZLWBN4ja9W2uRf4eq4veWNJ6wEPAgdLGqDskewHllHfwcB0SauRPUki7xBJLanOHwNeSGWfkOZH0lbpcU1mZXPL1youImalFuSNbU9pAL4bES9KGgPcKWkBWbfF4A5W8c/AWEnHAMuAEyJivKRH0lCuu1O/7yeA8anlPR/4akQ8IelmYDLZfXEfK6PK3wP+DMxK/+fr9AbwKLAGcHx68sQvyPqCn1BW+Czg4PK2jlnGN9YxMyuAux3MzArg4GtmVgAHXzOzAjj4mpkVwMHXzKwADr5mZgVw8DUzK8D/B825WVftStLJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd574730630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if do_run(\"xgb_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"xgb_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, counts, xgboost\", \n",
    "                                               xgb.XGBClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"xgb_sc\")\n",
    "\n",
    "\n",
    "# if do_run(\"xgb_sc\"):\n",
    "#     register_run(\"xgb1\",\n",
    "#              \"character and word n-grams, with tf-idf feature vectors, used in a extreme gradient boost setting\",\n",
    "#              [])\n",
    "    \n",
    "#     xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "#     parameters = {'nthread':[1], #when use hyperthread, xgboost may become slower\n",
    "#                   'objective':['binary:logistic'],\n",
    "#                   'learning_rate': [0.05], #so called `eta` value\n",
    "#                   'max_depth': [6],\n",
    "#                   'min_child_weight': [11],\n",
    "#                   'silent': [1],\n",
    "#                   'subsample': [0.8],\n",
    "#                   'colsample_bytree': [0.7],\n",
    "#                   'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "#                   'missing':[-999],\n",
    "#                   'seed': [1337]}\n",
    "\n",
    "#     from sklearn.cross_validation import *\n",
    "#     clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "#                        cv=StratifiedKFold(y_training[use], n_folds=5, shuffle=True), \n",
    "#                        scoring='accuracy',\n",
    "#                        verbose=2, refit=True)\n",
    "\n",
    "#     #bst = clf.fit(X_training, y_training)\n",
    "#     #xgb_model = xgb.XGBClassifier(nthread=1, objective='binary:logistic', learning_rate=0.05, max_depth=6, min_child_weight=11, missing=-999, n_estimators=1000, subsample=0.8, colsample_bytree=0.7)\n",
    "\n",
    "#     #xgb.plot_importance(xgb_model)\n",
    "#     #plt.show()\n",
    "    \n",
    "#     #update_stats(\"xgb1\", y_dev, y_pred, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "if do_run(\"rfo_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"rfo_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, tfidf, random forests\", \n",
    "                                            RandomForestClassifier(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"rfo_st\")\n",
    "\n",
    "if do_run(\"mnb_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"mnb_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, tfidf, mult naive bayes\", \n",
    "                                            MultinomialNB(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"mnb_st\") \n",
    "\n",
    "if do_run(\"knn_st\"):\n",
    "    for n in range(1,7):\n",
    "        for k in [100, 1000, 10000, 100000]:\n",
    "             print(n, k, SelectCountVectorFeatures(\"knn_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, tfidf, knn\", \n",
    "                                                   KNeighborsClassifier(n_neighbors=n), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"knn_st\") \n",
    "\n",
    "if do_run(\"sgd_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"sgd_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, tfidf, sgd\", \n",
    "                                               SGDClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"sgd_st\")\n",
    "\n",
    "if do_run(\"xgb_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"xgb_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, tfidf, xgboost\", \n",
    "                                               xgb.XGBClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"xgb_st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf-based features\n",
    "### All character and word n-gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if do_run(\"svc1\"):\n",
    "#     svc2 = TfidfVectorFeatures(\"svc1\", \"c&w n-grams, tfidf, (l)svc\", SVC(kernel='linear')).run()\n",
    "# summarise_run(\"svc1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "if do_run(\"lsc1\"):\n",
    "    lsc1 = TfidfVectorFeatures(\"lsc1\", \"c&w n-grams, tfidf, lsvc\", LinearSVC()).run()\n",
    "summarise_run(\"lsc1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "if do_run(\"bag1\"):\n",
    "    bag1 = TfidfVectorFeatures(\"bag1\", \"c&w n-grams, tfidf, bagging svc\", BaggingClassifier(LinearSVC(), n_estimators=10, max_samples=0.1)).run()\n",
    "summarise_run(\"bag1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#    for alpha in tqdm(10.0 ** -np.arange(1, 7), desc=\"alpha\"):\n",
    "#        for hls in tqdm([(5,2), (5,5), (10,2), (10,5), (50,2), (50,5), (50,10)], desc=\"hls\"):\n",
    "\n",
    "if do_run(\"mlp1\"):\n",
    "    mlp1 = TfidfVectorFeatures(\"mlp1\", \"c&w n-grams, tfidf, mlp adam\", MLPClassifier(solver='adam', alpha=alpha, hidden_layer_sizes=hls, random_state=1)).run()\n",
    "summarise_run(\"mlp1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "from abc import ABCMeta\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "\n",
    "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.svm_ = [] # fuggly\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # so we don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "\n",
    "        # Count raw events from data\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
    "                                 dtype=np.float64)\n",
    "        self._compute_ratios(X, Y)\n",
    "\n",
    "        # flugglyness\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
    "            Y_i = Y[:,i]\n",
    "            svm.fit(X_i, Y_i)\n",
    "            self.svm_.append(svm) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_effective_classes = self.class_count_.shape[0]\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        D = np.zeros((n_effective_classes, n_examples))\n",
    "\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            D[i] = self.svm_[i].decision_function(X_i)\n",
    "        \n",
    "        return self.classes_[np.argmax(D, axis=0)]\n",
    "        \n",
    "    def _compute_ratios(self, X, Y):\n",
    "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "\n",
    "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
    "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
    "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
    "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
    "        check_array(self.ratios_)\n",
    "        self.ratios_ = sparse.csr_matrix(self.ratios_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "if do_run(\"nbs1\"):\n",
    "    register_run(\"nbs1\",\n",
    "             \"character and word n-grams, with tf-idf feature vectors, used in a naive bayes/svm setting\",\n",
    "             ['C', 'min_cn', 'max_cn', 'min_n', 'max_n'])\n",
    "    \n",
    "    for C in tqdm([0.01, 0.1, 1.0], desc=\"C\"):\n",
    "\n",
    "        for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "            for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "\n",
    "                for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "                    for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "\n",
    "                        steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                                 ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                                ]\n",
    "\n",
    "                        union = FeatureUnion(steps)\n",
    "\n",
    "                        pipeline = Pipeline([\n",
    "                            ('union', union),\n",
    "                            ('nbs', NBSVM(C=C)),\n",
    "                        ])\n",
    "\n",
    "                        model = pipeline.fit(X_training, y_training)\n",
    "                        y_pred = model.predict(X_dev)\n",
    "\n",
    "                        update_stats(\"nbs1\", y_dev, y_pred, {'C': C, 'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n",
    "\n",
    "summarise_run(\"nbs1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if do_run(\"rfo2\"):\n",
    "    \n",
    "    register_run(\"rfo1\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a random forest setting\",\n",
    "             ['count'])\n",
    "    \n",
    "    for count in tqdm(range(1, 10000, 5000), desc=\"# llh counts\"):\n",
    "\n",
    "        steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                 ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                ]\n",
    "\n",
    "        union = FeatureUnion(steps)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('union', union),\n",
    "            ('rfo', RandomForestClassifier()),\n",
    "        ])\n",
    "\n",
    "        model = pipeline.fit(X_training, y_training)\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(\"rfo2\", y_dev, y_pred, {'count': count})\n",
    "\n",
    "summarise_run(\"rfo2\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select k-best character and word n-gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if do_run(\"rfo2\"):\n",
    "    \n",
    "    register_run(\"rfo1\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a random forest setting\",\n",
    "             ['count'])\n",
    " \n",
    "    print(\"Extracting features\")\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,6),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "    X_training_vec = vectorizer.fit_transform(X_training)\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_training_vect.shape)\n",
    "\n",
    "    print(\"Extracting from test data\")\n",
    "    X_dev_vec = vectorizer.transform(X_dev)\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_dev_vect.shape)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    for k in range(1,100000, 1000):\n",
    "\n",
    "\n",
    "        print(\"Extracting %d best features by a chi-squared test\" % k)\n",
    "        ch2 = SelectKBest(chi2, k=k)\n",
    "        #print(chi2(X_training_vect, y_training))\n",
    "        X_training_vect = ch2.fit_transform(X_training_vec, y_training)\n",
    "        X_dev_vect = ch2.transform(X_dev_vec)\n",
    "#         print(\"n_samples: %d, n_features: %d\" % X_training_vect.shape)\n",
    "#         print(\"n_samples: %d, n_features: %d\" % X_dev_vect.shape)\n",
    "\n",
    "\n",
    "        clf = RandomForestClassifier(max_depth=2)\n",
    "        clf.fit(X_training_vect, y_training)\n",
    "        print(k, clf.score(X_dev_vect, y_dev ))\n",
    "    \n",
    "#     for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "#         for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "\n",
    "#             for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "#                 for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "\n",
    "#                     steps = [('char', )),\n",
    "#                              ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "#                             ]\n",
    "\n",
    "#                     union = FeatureUnion(steps)\n",
    "\n",
    "#                     pipeline = Pipeline([\n",
    "#                         ('union', union),\n",
    "#                         ('rfo', RandomForestClassifier()),\n",
    "#                     ])\n",
    "\n",
    "#                     model = pipeline.fit(X_training, y_training)\n",
    "#                     print(model)\n",
    "\n",
    "#                     ch2 = SelectKBest(chi2, k=1000)\n",
    "#                     X_training_new = ch2.fit_transform(X_training, y_training)\n",
    "#                     X_dev_new = ch2.transform(X_dev)\n",
    "\n",
    "#                     y_pred = model.predict(X_dev)\n",
    "\n",
    "#                     update_stats(\"rfo2\", y_dev, y_pred, {'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n",
    "\n",
    "summarise_run(\"rfo2\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loglikelihood-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a new vectoriser based on the loglikelihood values as computed by colibricore-loglikelihood.\n",
    "# Based on some thresholds (n-gram occurrence >= 2, 1 <= n <= 3 -grams), it computes the occurrence counts,\n",
    "# their frequency and the corresponding loglikelihood scores.\n",
    "# These scores are sorted, and then this vectoriser takes the top m patterns, and marks whether that pattern\n",
    "# is presented in the given set.\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LLHbasedBinaryVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, count=1000):\n",
    "        self.llh_1000 = []\n",
    "        with open('data/DUT_BEL.t5m1l6.llh_t', 'r') as f:\n",
    "            for n, line in enumerate(f):\n",
    "                self.llh_1000.append(line.split(\"\\t\")[0])\n",
    "                if n >= count:\n",
    "                    break\n",
    "    \n",
    "    def llh_binary_countvectorizer(self, line):\n",
    "        values = []\n",
    "        for k in self.llh_1000:\n",
    "            values.append(1*(k in line))\n",
    "        return values\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        result = []\n",
    "        for l in df:\n",
    "            result.append(self.llh_binary_countvectorizer(l))\n",
    "        return result\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if do_run(\"svc3\"):\n",
    "    \n",
    "    register_run(\"svc3\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a linear support vector classifier\",\n",
    "             ['count'])\n",
    "    \n",
    "    for count in tqdm(range(1, 10000, 500), desc=\"# llh counts\"):\n",
    "\n",
    "        steps = [('llh', LLHbasedBinaryVectorizer(count=count)),\n",
    "                 #('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                ]\n",
    "\n",
    "        union = FeatureUnion(steps)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('union', union),\n",
    "            ('svc', SVC(kernel='linear')),\n",
    "        ])\n",
    "\n",
    "        model = pipeline.fit(X_training, y_training)\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(\"svc3\", y_dev, y_pred, {'count': count})\n",
    "        \n",
    "summarise_run(\"svc3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if do_run(\"mnb2\"):\n",
    "    \n",
    "    register_run(\"mnb2\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a multinominal naive bayes setting\",\n",
    "             ['count'])\n",
    "    \n",
    "    for count in tqdm(range(1, 10000, 500), desc=\"# llh counts\"):\n",
    "\n",
    "        steps = [('llh', LLHbasedBinaryVectorizer(count=count)),\n",
    "                 #('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                ]\n",
    "\n",
    "        union = FeatureUnion(steps)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('union', union),\n",
    "            ('mnb', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "        model = pipeline.fit(X_training, y_training)\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(\"mnb2\", y_dev, y_pred, {'count': count})\n",
    "        \n",
    "summarise_run(\"mnb2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if do_run(\"knn2\"):\n",
    "    \n",
    "    register_run(\"knn2\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a k nearest neighbours setting\",\n",
    "             ['neighbours', 'count'])\n",
    "    \n",
    "    for neighbours in tnrange(1,7, desc=\"neighbours\"):\n",
    "        for count in tqdm(range(1, 10000, 500), desc=\"# llh counts\"):\n",
    "\n",
    "            steps = [('llh', LLHbasedBinaryVectorizer(count=count)),\n",
    "                     #('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                    ]\n",
    "\n",
    "            union = FeatureUnion(steps)\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('union', union),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=neighbours)),\n",
    "            ])\n",
    "\n",
    "            model = pipeline.fit(X_training, y_training)\n",
    "            y_pred = model.predict(X_dev)\n",
    "\n",
    "            update_stats(\"knn2\", y_dev, y_pred, {'neighbours': neighbours, 'count': count})\n",
    "\n",
    "summarise_run(\"knn2\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if do_run(\"rfo1\"):\n",
    "    \n",
    "    register_run(\"rfo1\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a random forest setting\",\n",
    "             ['count'])\n",
    "    \n",
    "    for count in tqdm(range(1, 10000, 500), desc=\"# llh counts\"):\n",
    "\n",
    "        steps = [('llh', LLHbasedBinaryVectorizer(count=count)),\n",
    "                 #('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                ]\n",
    "\n",
    "        union = FeatureUnion(steps)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('union', union),\n",
    "            ('rfo', RandomForestClassifier()),\n",
    "        ])\n",
    "\n",
    "        model = pipeline.fit(X_training, y_training)\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(\"rfo1\", y_dev, y_pred, {'count': count})\n",
    "\n",
    "summarise_run(\"rfo1\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "if do_run(\"fat1\"):\n",
    "    \n",
    "    register_run(\"fat1\",\n",
    "             \"character and word n-grams, with embeddings, used in a fasttext (w2v sg) setting\",\n",
    "             [])\n",
    "\n",
    "    with open('fasttext.train.txt', 'w') as f:\n",
    "        for line, label in zip(X_training, y_training):\n",
    "            f.write(line + \" __language__\" + label + \"\\n\")\n",
    "\n",
    "    ft_classifier = fasttext.supervised('fasttext.train.txt', 'model', \n",
    "                                        min_count=1, \n",
    "                                        word_ngrams=3, \n",
    "                                        minn=7, \n",
    "                                        maxn=7, \n",
    "                                        thread=2, \n",
    "                                        label_prefix='__language__')\n",
    "    ft_predictions = ft_classifier.predict(X_dev)\n",
    "    \n",
    "    update_stats(\"fat1\", y_dev, ft_predictions, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp\n",
    "mlp is sensitive to feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if do_run(\"xgb1\"):\n",
    "    register_run(\"xgb1\",\n",
    "             \"character and word n-grams, with tf-idf feature vectors, used in a extreme gradient boost setting\",\n",
    "             [])\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "    parameters = {'nthread':[1], #when use hyperthread, xgboost may become slower\n",
    "                  'objective':['binary:logistic'],\n",
    "                  'learning_rate': [0.05], #so called `eta` value\n",
    "                  'max_depth': [6],\n",
    "                  'min_child_weight': [11],\n",
    "                  'silent': [1],\n",
    "                  'subsample': [0.8],\n",
    "                  'colsample_bytree': [0.7],\n",
    "                  'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "                  'missing':[-999],\n",
    "                  'seed': [1337]}\n",
    "\n",
    "    from sklearn.cross_validation import *\n",
    "    clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                       cv=StratifiedKFold(y_training[use], n_folds=5, shuffle=True), \n",
    "                       scoring='accuracy',\n",
    "                       verbose=2, refit=True)\n",
    "\n",
    "    #bst = clf.fit(X_training, y_training)\n",
    "    #xgb_model = xgb.XGBClassifier(nthread=1, objective='binary:logistic', learning_rate=0.05, max_depth=6, min_child_weight=11, missing=-999, n_estimators=1000, subsample=0.8, colsample_bytree=0.7)\n",
    "\n",
    "    #xgb.plot_importance(xgb_model)\n",
    "    #plt.show()\n",
    "    \n",
    "    #update_stats(\"xgb1\", y_dev, y_pred, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
