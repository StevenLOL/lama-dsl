{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains the full pipeline for our submissions to the 2018 VarDial Evaluation Campaign, in particular to the shared task for Discriminating between Dutch and Flemish in subtitles (DFS). We are participating only in the closed training track. See [the official page](http://alt.qcri.org/vardial2018/index.php?id=campaign) for more info.\n",
    "\n",
    "Running the whole stack takes a really long time, especially tuning the parameters for the models, the stacks and blends. There is a cache mechanism in place, so you don't have run it all yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ucto\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Here you can enter the parts that you want to run. The identifiers are the run names, and you can find them further\n",
    "# in the notebook. Use \"all\" to run all of them; this option overrules all other run names. Also note that it takes a\n",
    "# seriously large amount of time to run them all.\n",
    "\n",
    "_run = set([])\n",
    "_use_subset = False\n",
    "_show_graphics = True\n",
    "\n",
    "import sys\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # running in console\n",
    "    _show_graphics = False\n",
    "    from tqdm import tqdm as tqdm, trange as tnrange\n",
    "    \n",
    "    for v in sys.argv:\n",
    "        if v.startswith(\"r:\"):\n",
    "            _run.add(v[2:])\n",
    "        elif v == \"o:subset\":\n",
    "            _use_subset = True\n",
    "else:\n",
    "    # running in notebook\n",
    "    from tqdm import tqdm_notebook as tqdm, tnrange as tnrange\n",
    "    \n",
    "    _run.add(\"all\")\n",
    "    \n",
    "    _use_subset = True\n",
    "    pass\n",
    "\n",
    "def do_run(name):\n",
    "    return name in _run or \"all\" in _run\n",
    "\n",
    "_run_stats = {}\n",
    "\n",
    "import os.path\n",
    "def register_run(name, description, params):\n",
    "    if name not in _run_stats:\n",
    "        # if can be loaded from file\n",
    "        if os.path.exists('cache/' + name + '.json'):\n",
    "            with open('cache/' + name + '.json', 'r') as f:\n",
    "                run_object = json.load(f)\n",
    "                _run_stats[name] = {}\n",
    "                _run_stats[name]['description'] = run_object['description']\n",
    "#                 _run_stats[name]['params'] = run_object['params']\n",
    "                _run_stats[name]['best'] = run_object['best']\n",
    "                _run_stats[name]['best_predictions'] = np.array(run_object['best_predictions'])\n",
    "#                 print(np.matrix(run_object['best_predictions']))\n",
    "                _run_stats[name]['orig_predictions'] = np.array(run_object['orig_predictions'])\n",
    "                _run_stats[name]['history'] = run_object['history']\n",
    "        else:\n",
    "            _run_stats[name] = {}\n",
    "            _run_stats[name]['description'] = description\n",
    "#             _run_stats[name]['params'] = {}\n",
    "#             for param in params:\n",
    "#                 _run_stats[name]['params'][param] = ''\n",
    "\n",
    "            _run_stats[name]['best'] = 0\n",
    "            _run_stats[name]['best_predictions'] = []\n",
    "            _run_stats[name]['orig_predictions'] = []\n",
    "            _run_stats[name]['history'] = []\n",
    "\n",
    "def update_stats(name, y_test, y_pred, params):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    _run_stats[name]['history'].append(accuracy)\n",
    "    if accuracy > _run_stats[name]['best']:\n",
    "        _run_stats[name]['best'] = accuracy\n",
    "#         for param in params:\n",
    "#             _run_stats[name]['params'][param] = params[param]\n",
    "        _run_stats[name]['best_predictions'] = y_pred\n",
    "        _run_stats[name]['orig_predictions'] = y_test\n",
    "        \n",
    "        with open(\"cache/\" + name + \".json\", 'w') as f:\n",
    "            json.dump({'best_predictions': _run_stats[name]['best_predictions'].tolist(),\n",
    "                       'description': _run_stats[name]['description'],\n",
    "#                        'params': _run_stats[name]['params'],\n",
    "                       'history': _run_stats[name]['history'],\n",
    "                       'orig_predictions': _run_stats[name]['orig_predictions'].tolist(),\n",
    "                       'best': _run_stats[name]['best']\n",
    "                      }, f)\n",
    "#         with open('cache/' + name + '.pickle', 'w') as f:\n",
    "#             pickle.dump(_run_stats[name], f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "def summarise_run(name):\n",
    "    if name not in _run_stats:\n",
    "        print(\"Name %s is not registrered.\" % (name))\n",
    "        return\n",
    "    \n",
    "    print(\"Model:    \\n\\t%s\" % (name))\n",
    "    #print(\"Settings: \\n\\t%s\" % (_run_stats[name]['params']))\n",
    "    print(\"Accuracy: \\n\\t%s\" % (_run_stats[name]['best']))\n",
    "\n",
    "    cm = confusion_matrix(_run_stats[name]['orig_predictions'], _run_stats[name]['best_predictions'])\n",
    "    \n",
    "    if _show_graphics:\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm, classes=[\"BEL\", \"DUT\"], title=name)# + \" \" + str(_run_stats[name]['params']))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(cm)       \n",
    "\n",
    "def summarise_all():\n",
    "    for name in _run_stats.keys():\n",
    "        summarise_run(name)\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(file):\n",
    "    with open(file) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucto_config = \"tokconfig-nld\"\n",
    "tokeniser = ucto.Tokenizer(ucto_config, sentenceperlineinput=True, sentencedetection=False, paragraphdetection=False)\n",
    "\n",
    "# We read the file with ucto and tokenise it according to its default Dutch tokenisation scheme, which is rule-based\n",
    "# and definitely better than a plain whitespace tokeniser from sklearn. Afterwards we concatenate the tokens back to a \n",
    "# whitespace seperated line, which can then be normally processed with sklearn's tokenisers.\n",
    "def read_data(file):\n",
    "    text = {}\n",
    "    with open(file) as f:\n",
    "        for line in tqdm(f):\n",
    "            sentence, language = line.strip().split(\"\\t\")\n",
    "            tokeniser.process(sentence)\n",
    "\n",
    "            if language not in text:\n",
    "                text[language] = []\n",
    "\n",
    "            current_line = []\n",
    "            for token in tokeniser:\n",
    "                current_line.append(str(token))\n",
    "                if token.isendofsentence():\n",
    "                    #print(current_line)\n",
    "                    text[language].append(\" \".join(current_line))\n",
    "                    current_line = []\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading development set from pickle.\n",
      "development set\n",
      "\t LAN\t size \t avg length\n",
      "\t BEL \t 250 \t 40.456\n",
      "\t DUT \t 250 \t 40.088\n",
      "Done reading training set from pickle.\n",
      "training set\n",
      "\t LAN\t size \t avg length\n",
      "\t BEL \t 150000 \t 40.273626666666665\n",
      "\t DUT \t 150000 \t 40.37152\n"
     ]
    }
   ],
   "source": [
    "# If this is the first run, then we have to tokenise the text. In other cases we probably have saved a pickled version\n",
    "# somewhere. If not, we will tokenise the text anyway. No worries.\n",
    "\n",
    "# First the development set\n",
    "try:\n",
    "    with open('data/dev.txt.pickle', 'rb') as f:\n",
    "        _l_dev_text = pickle.load(f)\n",
    "        print(\"Done reading development set from pickle.\")\n",
    "except IOError:\n",
    "    _l_dev_text = read_data('data/dev.txt')\n",
    "    print(\"Done tokenising development set.\")\n",
    "    with open('data/dev.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_dev_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing development set from pickle.\")\n",
    "\n",
    "print(\"development set\")\n",
    "print(\"\\t LAN\\t size \\t avg length\")\n",
    "for l in _l_dev_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_dev_text[l]), \"\\t\", sum([len(x.split()) for x in _l_dev_text[l]])/len(_l_dev_text[l]))\n",
    "\n",
    "# And then the training set. This takes bit more time...\n",
    "try:\n",
    "    with open('data/train.txt.pickle', 'rb') as f:\n",
    "        _l_trn_text = pickle.load(f)\n",
    "        print(\"Done reading training set from pickle.\")\n",
    "except IOError:\n",
    "    _l_trn_text = read_data('data/train.txt')\n",
    "    print(\"Done tokenising training set.\")\n",
    "    with open('data/train.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_trn_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing training set from pickle.\")\n",
    "\n",
    "print(\"training set\")\n",
    "print(\"\\t LAN\\t size \\t avg length\")\n",
    "for l in _l_trn_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_trn_text[l]), \"\\t\", sum([len(x.split()) for x in _l_trn_text[l]])/len(_l_trn_text[l]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert the training and development material into the right shape, and make sure that we also keep track of\n",
    "# the labels.\n",
    "\n",
    "X_training = []\n",
    "y_training = []\n",
    "for l in _l_trn_text.keys():\n",
    "    for s in _l_trn_text[l]:\n",
    "        X_training.append(s)\n",
    "        y_training.append(l)\n",
    "X_training = np.array(X_training)\n",
    "y_training = np.array(y_training)\n",
    "\n",
    "\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "for l in _l_dev_text.keys():\n",
    "    for s in _l_dev_text[l]:\n",
    "        X_dev.append(s)\n",
    "        y_dev.append(l)\n",
    "X_dev = np.array(X_dev)\n",
    "y_dev = np.array(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/' + 'dev' + '.POS.txt') or not os.path.exists('data/' + 'train' + '.POS.txt'):\n",
    "    import frog\n",
    "\n",
    "    frog = frog.Frog(frog.FrogOptions(parser=False))\n",
    "\n",
    "    for t in ['dev', 'train']:\n",
    "        with open('data/' + t + '.POS.txt', 'w') as out:\n",
    "            with open('data/' + t + '.txt', 'r') as f:\n",
    "                for line in f:\n",
    "                    sentence, tag = line.strip().split(\"\\t\")\n",
    "                    froggo = frog.process(sentence)\n",
    "                    postext = []\n",
    "                    for w in froggo:\n",
    "                        postext.append(w['pos'].split(\"(\")[0])\n",
    "                    out.write(\" \".join(postext) + \"\\t\" + tag + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pos texts into X_pos_training etc.\n",
    "\n",
    "X_pos_training = []\n",
    "y_pos_training = []\n",
    "with open('data/train.POS.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sentence, tag = line.strip().split(\"\\t\")\n",
    "        X_pos_training.append(sentence)\n",
    "        y_pos_training.append(tag)\n",
    "X_pos_training = np.array(X_pos_training)\n",
    "y_pos_training = np.array(y_pos_training)\n",
    "\n",
    "X_pos_dev = []\n",
    "y_pos_dev = []\n",
    "with open('data/dev.POS.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sentence, tag = line.strip().split(\"\\t\")\n",
    "        X_pos_dev.append(sentence)\n",
    "        y_pos_dev.append(tag)\n",
    "X_pos_dev = np.array(X_pos_dev)\n",
    "y_pos_dev = np.array(y_pos_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training subset\n"
     ]
    }
   ],
   "source": [
    "# Sometimes for testing whether some code words, you might want to use a subset. Use this one. Or another one. I don't\n",
    "# care. \n",
    "\n",
    "import random\n",
    "#use = random.sample(range(1, 299999), 100000)\n",
    "use = random.sample(range(1, 299999), 500)\n",
    "\n",
    "if _use_subset:   \n",
    "    X_training = X_training[use]\n",
    "    y_training = y_training[use]\n",
    "    \n",
    "    print(\"training subset\")\n",
    "    #print(\"\\t LAN\\t size \\t avg length\")\n",
    "    #for l in _l_trn_text.keys():\n",
    "    #    print(\"\\t\", l, \"\\t\", len(_l_trn_text[l]), \"\\t\", sum([len(x.split()) for x in _l_trn_text[l]])/len(_l_trn_text[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerFeatures():\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, features, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6):\n",
    "        self.run_name = name\n",
    "        self.run_description = description\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        self.features = features\n",
    "        self.feature_selection = feature_selection\n",
    "        self.scaler = scaler\n",
    "    \n",
    "        self.run_min_cn = min_cn\n",
    "        self.run_max_cn = max_cn\n",
    "        \n",
    "        self.run_min_n = min_n\n",
    "        self.run_max_n = max_n\n",
    "    \n",
    "        register_run(self.run_name, self.run_description, [])\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        #print(\"Classifier:\", self.classifier)\n",
    "        #print(\"Feature extraction:\", self.features)\n",
    "        #print(\"Feature selection:\", self.feature_selection)\n",
    "        #print(\"Feature scaler:\", self.scaler)\n",
    "                \n",
    "        self.pipeline = Pipeline([\n",
    "            ('features',   FeatureUnion(self.features)),\n",
    "            ('scaler',     self.scaler),\n",
    "            ('selection',  self.feature_selection),\n",
    "            ('classifier', self.classifier),\n",
    "        ])\n",
    "\n",
    "        model = self.pipeline.fit(X_training, y_training)\n",
    "        self.y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(self.run_name, y_dev, self.y_pred, {'min_cn': self.run_min_cn, 'max_cn': self.run_max_cn, 'min_n': self.run_min_n, 'max_n': self.run_max_n})\n",
    "        \n",
    "        return accuracy_score(y_dev, self.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "class CountVectorFeatures(VectorizerFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6):\n",
    "        \n",
    "        features = [('char', CountVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(CountVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, features, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "        \n",
    "class SelectCountVectorFeatures(CountVectorFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6, k = 100):\n",
    "        features = [('char', CountVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(SelectCountVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "        self.k = k\n",
    "        self.feature_selection = SelectKBest(chi2, k=k)\n",
    "\n",
    "##\n",
    "        \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class TfidfVectorFeatures(VectorizerFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6):\n",
    "        \n",
    "        features = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(TfidfVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, features, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "        \n",
    "class SelectTfidfVectorFeatures(TfidfVectorFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6, k = 100):\n",
    "        features = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(SelectTfidfVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "        self.k = k\n",
    "        self.feature_selection = SelectKBest(chi2, k=k)\n",
    "\n",
    "##\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class POSVectorFeatures(VectorizerFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_n = 2, max_n = 6):\n",
    "        \n",
    "        features = [('words', TfidfVectorizer(analyzer='word', use_idf=False, ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(POSVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, features, feature_selection, scaler, min_n, max_n)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "        \n",
    "class SelectPOSVectorFeatures(POSVectorFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_n = 2, max_n = 6, k = 100):\n",
    "        features = [('words', TfidfVectorizer(analyzer='word', use_idf=False, ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(SelectPOSVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, feature_selection, scaler, min_n, max_n)\n",
    "\n",
    "        self.k = k\n",
    "        self.feature_selection = SelectKBest(chi2, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "from abc import ABCMeta\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "\n",
    "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.svm_ = [] # fuggly\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # so we don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "\n",
    "        # Count raw events from data\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
    "                                 dtype=np.float64)\n",
    "        self._compute_ratios(X, Y)\n",
    "\n",
    "        # flugglyness\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
    "            Y_i = Y[:,i]\n",
    "            svm.fit(X_i, Y_i)\n",
    "            self.svm_.append(svm) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_effective_classes = self.class_count_.shape[0]\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        D = np.zeros((n_effective_classes, n_examples))\n",
    "\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            D[i] = self.svm_[i].decision_function(X_i)\n",
    "        \n",
    "        return self.classes_[np.argmax(D, axis=0)]\n",
    "        \n",
    "    def _compute_ratios(self, X, Y):\n",
    "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "\n",
    "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
    "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
    "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
    "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
    "        check_array(self.ratios_)\n",
    "        self.ratios_ = sparse.csr_matrix(self.ratios_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# 100 0.658\n",
    "# 1000 0.658\n",
    "# 10000 0.648\n",
    "# 100000 0.656\n",
    "# Model:    \n",
    "#         rfo_sc\n",
    "# Settings: \n",
    "#         {'max_n': 6, 'max_cn': 8, 'min_cn': 1, 'min_n': 1}\n",
    "# Accuracy: \n",
    "#         0.658\n",
    "# [[190  60]\n",
    "#  [111 139]]\n",
    "if do_run(\"rfo_sc\"):\n",
    "    for k in [1000]:\n",
    "        model = SelectCountVectorFeatures(\"rfo_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                          \"c&w n-grams, counts, random forests\", \n",
    "                                          RandomForestClassifier(), \n",
    "                                          k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"rfo_sc\")\n",
    "\n",
    "# 100 0.572\n",
    "# 1000 0.558\n",
    "# 10000 0.572\n",
    "# 100000 0.586\n",
    "# Model:    \n",
    "#         mnb_sc\n",
    "# Settings: \n",
    "#         {'min_n': 1, 'max_n': 6, 'min_cn': 1, 'max_cn': 8}\n",
    "# Accuracy: \n",
    "#         0.586\n",
    "# [[154  96]\n",
    "#  [111 139]]\n",
    "if do_run(\"mnb_sc\"):\n",
    "    for k in [100000]:\n",
    "        model = SelectCountVectorFeatures(\"mnb_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, counts, mult naive bayes\", \n",
    "                                            MultinomialNB(), \n",
    "                                            k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"mnb_sc\") \n",
    "\n",
    "\n",
    "# 1 100 0.606\n",
    "# 1 1000 0.65\n",
    "# 1 10000 0.616\n",
    "# 1 100000 0.624        <-- best, but run was interrupted\n",
    "# 2 100 0.552\n",
    "# 2 1000 0.596\n",
    "# 2 10000 0.574\n",
    "# 2 100000 0.58\n",
    "# 3 100 0.522\n",
    "# 3 1000 0.55\n",
    "# 3 10000 0.57\n",
    "# 3 100000 0.58\n",
    "# 4 100 0.53\n",
    "# 4 1000 0.56\n",
    "# 4 10000 0.556\n",
    "# 4 100000 0.592\n",
    "# 5 100 0.536\n",
    "# 5 100 0.536\n",
    "# 5 1000 0.552\n",
    "# 5 10000 0.556\n",
    "# 5 100000 0.558\n",
    "# 6 100 0.538\n",
    "# 6 1000 0.576\n",
    "# 6 10000 0.544\n",
    "# 6 100000 0.542\n",
    "# Model:    \n",
    "#         knn_sc\n",
    "# Settings: \n",
    "#         {'min_n': 1, 'max_cn': 8, 'min_cn': 1, 'max_n': 6}\n",
    "# Accuracy: \n",
    "#         0.576\n",
    "# [[170  80]\n",
    "#  [132 118]]\n",
    "\n",
    "if do_run(\"knn_sc\"):\n",
    "    for n in [1]\n",
    "        for k in [100000]:\n",
    "            model = SelectCountVectorFeatures(\"knn_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, counts, knn\", \n",
    "                                                   KNeighborsClassifier(n_neighbors=n), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(n, k, model_acc)\n",
    "summarise_run(\"knn_sc\") \n",
    "\n",
    "# 100 0.5\n",
    "# 1000 0.57\n",
    "# 10000 0.558\n",
    "# 100000 0.628\n",
    "# Model:    \n",
    "#         sgd_sc\n",
    "# Settings: \n",
    "#         {'max_n': 6, 'max_cn': 8, 'min_cn': 1, 'min_n': 1}\n",
    "# Accuracy: \n",
    "#         0.628\n",
    "# [[175  75]\n",
    "#  [111 139]]\n",
    "if do_run(\"sgd_sc\"):\n",
    "    for k in [100000]:\n",
    "        model = SelectCountVectorFeatures(\"sgd_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, counts, sgd\", \n",
    "                                               SGDClassifier(), \n",
    "                                               k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"sgd_sc\")\n",
    "\n",
    "# 100 0.1 0.578\n",
    "# 1000 0.1 0.57\n",
    "# 10000 0.1 0.586\n",
    "# 100000 0.1 0.598\n",
    "# Model:    \n",
    "#         nbs_sc\n",
    "# Settings: \n",
    "#         {'max_cn': 8, 'min_n': 1, 'min_cn': 1, 'max_n': 6}\n",
    "# Accuracy: \n",
    "#         0.598\n",
    "# [[138 112]\n",
    "#  [ 89 161]]\n",
    "if do_run(\"nbs_sc\"):\n",
    "    for k in [100000]:\n",
    "        for C in [0.1]:\n",
    "            model = SelectCountVectorFeatures(\"nbs_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, counts, naive bayes/svm\", \n",
    "                                                   NBSVM(C=C), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(k, C, model_acc)\n",
    "summarise_run(\"nbs_sc\")\n",
    "\n",
    "# 100 0.602\n",
    "# 1000 0.6\n",
    "# 10000 0.594\n",
    "# 100000 0.592\n",
    "# Model:    \n",
    "#         xgb_sc\n",
    "# Settings: \n",
    "#         {'min_cn': 1, 'max_cn': 8, 'min_n': 1, 'max_n': 6}\n",
    "# Accuracy: \n",
    "#         0.602\n",
    "# [[169  81]\n",
    "#  [118 132]]\n",
    "if do_run(\"xgb_sc\"):\n",
    "    for k in [100]:\n",
    "        for lr in [0.05]:\n",
    "            for mcw in [1]:\n",
    "                for alpha in [0]:\n",
    "                    model = SelectCountVectorFeatures(\"xgb_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                                           \"c&w n-grams, counts, xgboost\", \n",
    "                                                           xgb.XGBClassifier(silent=0, learning_rate=lr, min_child_weight=mcw, reg_alpha=alpha), \n",
    "                                                           k=k)\n",
    "                    model_acc = model.run()\n",
    "                    print(k, lr, mcw, alpha, model_acc)\n",
    "summarise_run(\"xgb_sc\")\n",
    "\n",
    "if do_run(\"lsc_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1, 0.5, 1.0]:\n",
    "            model = SelectCountVectorFeatures(\"lsc_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, count, linear svc\", \n",
    "                                                   LinearSVC(C=C), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(k, C, model_acc)\n",
    "summarise_run(\"lsc_sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# 100 0.628\n",
    "# 1000 0.644\n",
    "# 10000 0.638\n",
    "# 100000 0.63\n",
    "# Model:    \n",
    "#         rfo_st\n",
    "# Settings: \n",
    "#         {'min_n': 1, 'max_n': 6, 'min_cn': 1, 'max_cn': 8}\n",
    "# Accuracy: \n",
    "#         0.644\n",
    "# [[181  69]\n",
    "#  [109 141]]\n",
    "if do_run(\"rfo_st\"):\n",
    "    for k in [1000]:\n",
    "        model = SelectTfidfVectorFeatures(\"rfo_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, tfidf, random forests\", \n",
    "                                            RandomForestClassifier(), \n",
    "                                            k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"rfo_st\")\n",
    "\n",
    "# 100 0.546\n",
    "# 1000 0.566\n",
    "# 10000 0.596\n",
    "# 100000 0.616\n",
    "# Model:    \n",
    "#         mnb_st\n",
    "# Settings: \n",
    "#         {'min_n': 1, 'min_cn': 1, 'max_n': 6, 'max_cn': 8}\n",
    "# Accuracy: \n",
    "#         0.616\n",
    "# [[163  87]\n",
    "#  [105 145]]\n",
    "if do_run(\"mnb_st\"):\n",
    "    for k in [100000]:\n",
    "        model = SelectTfidfVectorFeatures(\"mnb_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, tfidf, mult naive bayes\", \n",
    "                                            MultinomialNB(), \n",
    "                                            k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"mnb_st\") \n",
    "\n",
    "# 1 100 0.632\n",
    "# 1 1000 0.64\n",
    "# 1 10000 0.648       <-- best, but run was interrupted\n",
    "# 1 100000 0.636\n",
    "# 2 100 0.526\n",
    "# 2 1000 0.564\n",
    "# 2 10000 0.612\n",
    "# 2 100000 0.574\n",
    "# 3 100 0.558\n",
    "# 3 1000 0.6\n",
    "# 3 10000 0.612\n",
    "# 3 100000 0.552\n",
    "# 4 100 0.526\n",
    "# 4 1000 0.584\n",
    "# 4 10000 0.598\n",
    "# 4 100000 0.548\n",
    "# 5 100 0.584\n",
    "# 5 1000 0.59\n",
    "# 5 10000 0.626\n",
    "# 5 100000 0.538\n",
    "# 6 100 0.558\n",
    "# 6 1000 0.592\n",
    "# 6 10000 0.586\n",
    "# 6 100000 0.506\n",
    "# Model:    \n",
    "#         knn_st\n",
    "# Settings: \n",
    "#         {'min_cn': 1, 'max_cn': 8, 'min_n': 1, 'max_n': 6}\n",
    "# Accuracy: \n",
    "#         0.626\n",
    "# [[162  88]\n",
    "#  [ 99 151]]\n",
    "if do_run(\"knn_st\"):\n",
    "    for n in [1]:\n",
    "        for k in [10000]:\n",
    "            model = SelectTfidfVectorFeatures(\"knn_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, tfidf, knn\", \n",
    "                                                   KNeighborsClassifier(n_neighbors=n), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(n, k, model_acc)\n",
    "summarise_run(\"knn_st\") \n",
    "\n",
    "# 100 0.568\n",
    "# 1000 0.57\n",
    "# 10000 0.572\n",
    "# 100000 0.58\n",
    "# Model:    \n",
    "#         sgd_st\n",
    "# Settings: \n",
    "#         {'max_n': 6, 'min_n': 1, 'max_cn': 8, 'min_cn': 1}\n",
    "# Accuracy: \n",
    "#         0.58\n",
    "# [[140 110]\n",
    "#  [100 150]]\n",
    "if do_run(\"sgd_st\"):\n",
    "    for k in [100000]:\n",
    "        model = SelectTfidfVectorFeatures(\"sgd_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, tfidf, sgd\", \n",
    "                                               SGDClassifier(), \n",
    "                                               k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"sgd_st\")\n",
    "\n",
    "# 100 0.598\n",
    "# 1000 0.584\n",
    "# 10000 0.594\n",
    "# 100000 0.588\n",
    "# Model:    \n",
    "#         xgb_st\n",
    "# Settings: \n",
    "#         {'max_cn': 8, 'max_n': 6, 'min_n': 1, 'min_cn': 1}\n",
    "# Accuracy: \n",
    "#         0.598\n",
    "# [[163  87]\n",
    "#  [114 136]]\n",
    "if do_run(\"xgb_st\"):\n",
    "    for k in [100]:\n",
    "        model = SelectTfidfVectorFeatures(\"xgb_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, tfidf, xgboost\", \n",
    "                                               xgb.XGBClassifier(), \n",
    "                                               k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"xgb_st\")\n",
    "\n",
    "\n",
    "# 100 0.1 0.582\n",
    "# 1000 0.1 0.584\n",
    "# 10000 0.1 0.61\n",
    "# 100000 0.1 0.608\n",
    "# Model:    \n",
    "#         nbs_st\n",
    "# Settings: \n",
    "#         {'min_n': 1, 'max_cn': 8, 'max_n': 6, 'min_cn': 1}\n",
    "# Accuracy: \n",
    "#         0.61\n",
    "# [[133 117]\n",
    "#  [ 78 172]]\n",
    "if do_run(\"nbs_st\"):\n",
    "    for k in [10000]:\n",
    "        for C in [0.1]:\n",
    "            model = SelectTfidfVectorFeatures(\"nbs_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, tfidf, naive bayes/svm\", \n",
    "                                                   NBSVM(C=C), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(k, C, model_acc)\n",
    "summarise_run(\"nbs_st\")\n",
    "\n",
    "if do_run(\"xgb_st1\"):\n",
    "    for k in [100000]:\n",
    "        for lr in [0.05]:\n",
    "            for mcw in [1]:\n",
    "                for alpha in [0]:\n",
    "                    model = SelectTfidfVectorFeatures(\"xgb_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                                           \"c&w n-grams, tfidf, xgboost\", \n",
    "                                                           xgb.XGBClassifier(silent=0, learning_rate=lr, min_child_weight=mcw, reg_alpha=alpha), \n",
    "                                                           k=k)\n",
    "                    model_acc = model.run()\n",
    "                    print(k, lr, mcw, alpha, model_acc)\n",
    "summarise_run(\"xgb_st\")\n",
    "\n",
    "# 100 0.1 0.598\n",
    "# 100 0.5 0.586\n",
    "# 100 1.0 0.586\n",
    "# 1000 0.1 0.57\n",
    "# 1000 0.5 0.582\n",
    "# 1000 1.0 0.578\n",
    "# 10000 0.1 0.63\n",
    "# 10000 0.5 0.626\n",
    "# 10000 1.0 0.626\n",
    "# 100000 0.1 0.624\n",
    "# 100000 0.5 0.622\n",
    "# 100000 1.0 0.61\n",
    "# Model:    \n",
    "#         lsc_st\n",
    "# Settings: \n",
    "#         {'min_cn': 1, 'min_n': 1, 'max_cn': 8, 'max_n': 6}\n",
    "# Accuracy: \n",
    "#         0.63\n",
    "# [[166  84]\n",
    "#  [101 149]\n",
    "if do_run(\"lsc_st\"):\n",
    "    for k in [10000]:\n",
    "        for C in [0.1]:\n",
    "            model = SelectTfidfVectorFeatures(\"lsc_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, tfidf, linear svc\", \n",
    "                                                   LinearSVC(C=C), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(k, C, model_acc)\n",
    "summarise_run(\"lsc_st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "if do_run(\"rfo_pos\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        model = SelectPOSVectorFeatures(\"rfo_pos\", X_training, y_training, X_dev, y_dev, \n",
    "                                          \"pos n-grams, counts, random forests\", \n",
    "                                          RandomForestClassifier(), \n",
    "                                          k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"rfo_pos\")\n",
    "\n",
    "\n",
    "if do_run(\"mnb_pos\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        model = SelectPOSVectorFeatures(\"mnb_pos\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, counts, mult naive bayes\", \n",
    "                                            MultinomialNB(), \n",
    "                                            k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"mnb_pos\") \n",
    "\n",
    "\n",
    "if do_run(\"knn_pos\"):\n",
    "    for n in [1, 2, 3, 4, 5, 6]\n",
    "        for k in [100, 1000, 10000, 100000]:\n",
    "            model = SelectPOSVectorFeatures(\"knn_pos\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, counts, knn\", \n",
    "                                                   KNeighborsClassifier(n_neighbors=n), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(n, k, model_acc)\n",
    "summarise_run(\"knn_pos\") \n",
    "\n",
    "\n",
    "if do_run(\"sgd_pos\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        model = SelectPOSVectorFeatures(\"sgd_pos\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, counts, sgd\", \n",
    "                                               SGDClassifier(), \n",
    "                                               k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"sgd_pos\")\n",
    "\n",
    "\n",
    "if do_run(\"nbs_pos\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1]:\n",
    "            model = SelectPOSVectorFeatures(\"nbs_pos\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"pos n-grams, counts, naive bayes/svm\", \n",
    "                                                   NBSVM(C=C), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(k, C, model_acc)\n",
    "summarise_run(\"nbs_pos\")\n",
    "\n",
    "\n",
    "if do_run(\"xgb_pos\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for lr in [0.05]:\n",
    "            for mcw in [1]:\n",
    "                for alpha in [0]:\n",
    "                    model = SelectPOSVectorFeatures(\"xgb_pos\", X_training, y_training, X_dev, y_dev, \n",
    "                                                           \"pos n-grams, counts, xgboost\", \n",
    "                                                           xgb.XGBClassifier(silent=0, learning_rate=lr, min_child_weight=mcw, reg_alpha=alpha), \n",
    "                                                           k=k)\n",
    "                    model_acc = model.run()\n",
    "                    print(k, lr, mcw, alpha, model_acc)\n",
    "summarise_run(\"xgb_pos\")\n",
    "\n",
    "if do_run(\"lsc_pos\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1, 0.5, 1.0]:\n",
    "            model = SelectPOSVectorFeatures(\"lsc_pos\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"pos n-grams, count, linear svc\", \n",
    "                                                   LinearSVC(C=C), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(k, C, model_acc)\n",
    "summarise_run(\"lsc_pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loglikelihood-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a new vectoriser based on the loglikelihood values as computed by colibricore-loglikelihood.\n",
    "# Based on some thresholds (n-gram occurrence >= 2, 1 <= n <= 3 -grams), it computes the occurrence counts,\n",
    "# their frequency and the corresponding loglikelihood scores.\n",
    "# These scores are sorted, and then this vectoriser takes the top m patterns, and marks whether that pattern\n",
    "# is presented in the given set.\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "class LLHbasedBinaryVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, count=1000):\n",
    "        self.llh_1000 = []\n",
    "        with open('data/DUT_BEL.t5m1l6.llh_t', 'r') as f:\n",
    "            for n, line in enumerate(f):\n",
    "                self.llh_1000.append(line.split(\"\\t\")[0])\n",
    "                if n >= count:\n",
    "                    break\n",
    "    \n",
    "    def llh_binary_countvectorizer(self, line):\n",
    "        values = []\n",
    "        for k in self.llh_1000:\n",
    "            values.append(1*(k in line))\n",
    "        return lil_matrix(values)\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        result = csr_matrix((len(df),len(self.llh_1000)))\n",
    "        for r, l in enumerate(df):\n",
    "            result[r,:] = self.llh_binary_countvectorizer(l)\n",
    "        return result\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLHbasedFeatures():\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, k=1000):\n",
    "        self.run_name = name\n",
    "        self.run_description = description\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        self.features = [('llh', LLHbasedBinaryVectorizer(count=k)),\n",
    "                    #('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        self.feature_selection = feature_selection\n",
    "        self.scaler = scaler\n",
    "    \n",
    "        self.k = k\n",
    "    \n",
    "        register_run(self.run_name, self.run_description, [])\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        #print(\"Classifier:\", self.classifier)\n",
    "        #print(\"Feature extraction:\", self.features)\n",
    "        #print(\"Feature selection:\", self.feature_selection)\n",
    "        #print(\"Feature scaler:\", self.scaler)\n",
    "                \n",
    "        self.pipeline = Pipeline([\n",
    "            ('features',   FeatureUnion(self.features)),\n",
    "            ('scaler',     self.scaler),\n",
    "            ('selection',  self.feature_selection),\n",
    "            ('classifier', self.classifier),\n",
    "        ])\n",
    "\n",
    "        model = self.pipeline.fit(X_training, y_training)\n",
    "        self.y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(self.run_name, y_dev, self.y_pred, {'k': self.k})\n",
    "        \n",
    "        return accuracy_score(y_dev, self.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "if do_run(\"rfo_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        model = LLHbasedFeatures(\"rfo_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, llh, random forests\", \n",
    "                                            RandomForestClassifier(), \n",
    "                                            k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"rfo_llh\")\n",
    "\n",
    "if do_run(\"mnb_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        model = LLHbasedFeatures(\"mnb_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, llh, mult naive bayes\", \n",
    "                                            MultinomialNB(), \n",
    "                                            k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"mnb_llh\") \n",
    "\n",
    "if do_run(\"knn_llh\"):\n",
    "    for n in range(1,7,2):\n",
    "        for k in [100, 1000, 10000, 100000]:\n",
    "            model = LLHbasedFeatures(\"knn_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, llh, knn\", \n",
    "                                                   KNeighborsClassifier(n_neighbors=n), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(n, k, model_acc)\n",
    "summarise_run(\"knn_llh\") \n",
    "\n",
    "if do_run(\"sgd_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        model = LLHbasedFeatures(\"sgd_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, sgd\", \n",
    "                                               SGDClassifier(), \n",
    "                                               k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"sgd_llh\")\n",
    "\n",
    "if do_run(\"log_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        model = LLHbasedFeatures(\"log_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, logit\", \n",
    "                                               SGDClassifier(loss=\"log\"), \n",
    "                                               k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"log_llh\")\n",
    "\n",
    "if do_run(\"pct_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        model = LLHbasedFeatures(\"pct_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, perceptron\", \n",
    "                                               SGDClassifier(loss=\"perceptron\"), \n",
    "                                               k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"pct_llh\")\n",
    "\n",
    "if do_run(\"xgb_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        model = LLHbasedFeatures(\"xgb_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, xgboost\", \n",
    "                                               xgb.XGBClassifier(), \n",
    "                                               k=k)\n",
    "        model_acc = model.run()\n",
    "        print(k, model_acc)\n",
    "summarise_run(\"xgb_llh\")\n",
    "\n",
    "if do_run(\"nbs_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1]:\n",
    "            model = LLHbasedFeatures(\"nbs_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, llh, naive bayes/svm\", \n",
    "                                                   NBSVM(C=C), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(k, C, model_acc)\n",
    "summarise_run(\"nbs_llh\")\n",
    "\n",
    "if do_run(\"xgb_llh\"):\n",
    "    for k in [100000]:\n",
    "        for lr in [0.05]:\n",
    "            for mcw in [1]:\n",
    "                for alpha in [0]:\n",
    "                    model = LLHbasedFeatures(\"xgb_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                                           \"c&w n-grams, llh, xgboost\", \n",
    "                                                           xgb.XGBClassifier(silent=0, learning_rate=lr, min_child_weight=mcw, reg_alpha=alpha), \n",
    "                                                           k=k)\n",
    "                    model_acc = model.run()\n",
    "                    print(k, lr, mcw, alpha, model_acc)\n",
    "summarise_run(\"xgb_llh\")\n",
    "\n",
    "if do_run(\"lsc_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1, 0.5, 1.0]:\n",
    "            model = LLHbasedFeatures(\"lsc_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, llh, linear svc\", \n",
    "                                                   LinearSVC(C=C), \n",
    "                                                   k=k)\n",
    "            model_acc = model.run()\n",
    "            print(k, C, model_acc)\n",
    "summarise_run(\"lsc_llh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Model:    \n",
    "#         fat1\n",
    "# Settings: \n",
    "#         {}\n",
    "# Accuracy: \n",
    "#         0.692\n",
    "# [[162  88]\n",
    "#  [ 66 184]]\n",
    "if do_run(\"fat1\"):\n",
    "    \n",
    "    register_run(\"fat1\",\n",
    "             \"character and word n-grams, with embeddings, used in a fasttext (w2v sg) setting\",\n",
    "             [])\n",
    "\n",
    "    with open('fasttext.train.txt', 'w') as f:\n",
    "        for line, label in zip(X_training, y_training):\n",
    "            f.write(line + \" __language__\" + label + \"\\n\")\n",
    "\n",
    "    ft_classifier = fasttext.supervised('fasttext.train.txt', 'model', \n",
    "                                        min_count=1, \n",
    "                                        word_ngrams=5, \n",
    "                                        epoch=100,\n",
    "                                        minn=3, \n",
    "                                        maxn=7, \n",
    "                                        thread=2, \n",
    "                                        dim=250,\n",
    "                                        ws=5,\n",
    "                                        bucket= 2000000,\n",
    "                                        label_prefix='__language__')\n",
    "    ft_predictions = ft_classifier.predict(X_dev)\n",
    "    \n",
    "    update_stats(\"fat1\", y_dev, ft_predictions, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name mkn_lm is not registrered.\n",
      "4-gram model\n",
      "4-gram model\n",
      "Model:    \n",
      "\tmkn_lm\n",
      "Accuracy: \n",
      "\t0.66\n",
      "Confusion matrix, without normalization\n",
      "[[160  90]\n",
      " [ 80 170]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49d5fda588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "_run_stats.pop('mkn_lm', None)\n",
    "summarise_run(\"mkn_lm\")\n",
    "\n",
    "if do_run(\"mkn_lm\"):\n",
    "    \n",
    "    register_run(\"mkn_lm\",\n",
    "             \"word 4-grams, in a modified kneser ney setting\",\n",
    "             [])\n",
    "    \n",
    "    lm_bel = kenlm.LanguageModel('data/train.BEL.arpa')\n",
    "    print('{0}-gram model'.format(lm_bel.order))\n",
    "\n",
    "    lm_dut = kenlm.LanguageModel('data/train.DUT.arpa')\n",
    "    print('{0}-gram model'.format(lm_dut.order))\n",
    "\n",
    "    lm_result = []\n",
    "    for sent in X_dev:\n",
    "        if lm_bel.score(sent) < lm_dut.score(sent): # closer to zero is better\n",
    "            lm_result.append(\"DUT\")\n",
    "        else:\n",
    "            lm_result.append(\"BEL\")\n",
    "    \n",
    "    update_stats(\"mkn_lm\", y_dev, np.array(lm_result), {'order': lm_bel.order})\n",
    "    \n",
    "summarise_run(\"mkn_lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name mkn_lm2 is not registrered.\n",
      "4-gram model\n",
      "4-gram model\n",
      "Model:    \n",
      "\tmkn_lm2\n",
      "Accuracy: \n",
      "\t0.614\n",
      "Confusion matrix, without normalization\n",
      "[[151  99]\n",
      " [ 94 156]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49d587ef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "_run_stats.pop('mkn_lm2', None)\n",
    "summarise_run(\"mkn_lm2\")\n",
    "\n",
    "if do_run(\"mkn_lm2\"):\n",
    "    \n",
    "    register_run(\"mkn_lm2\",\n",
    "             \"word 4-grams, in a modified kneser ney setting, trained on both, overtrained on one\",\n",
    "             [])\n",
    "    \n",
    "    lm_bel = kenlm.LanguageModel('data/train.DUTBELBELBEL.arpa')\n",
    "    print('{0}-gram model'.format(lm_bel.order))\n",
    "\n",
    "    lm_dut = kenlm.LanguageModel('data/train.BELDUTDUTDUT.arpa')\n",
    "    print('{0}-gram model'.format(lm_dut.order))\n",
    "\n",
    "    lm_result = []\n",
    "    for sent in X_dev:\n",
    "        if lm_bel.score(sent) < lm_dut.score(sent): # closer to zero is better\n",
    "            lm_result.append(\"DUT\")\n",
    "        else:\n",
    "            lm_result.append(\"BEL\")\n",
    "    \n",
    "    update_stats(\"mkn_lm2\", y_dev, np.array(lm_result), {'order': lm_bel.order})\n",
    "    \n",
    "summarise_run(\"mkn_lm2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUT 18\n",
      "BEL 17\n",
      "ALL 27\n",
      "Model:    \n",
      "\tzlib\n",
      "Accuracy: \n",
      "\t0.536\n",
      "Confusion matrix, without normalization\n",
      "[[204  46]\n",
      " [186  64]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49d5d28630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import zlib\n",
    "\n",
    "if do_run(\"zlib\"):\n",
    "    register_run(\"zlib\",\n",
    "             \"zlib compression, level 9\",\n",
    "             [])\n",
    "\n",
    "    ds = \"\"\n",
    "    bs = \"\"\n",
    "    for s, l in zip(X_training, y_training):\n",
    "        if l == \"DUT\":\n",
    "            ds += l\n",
    "        else:\n",
    "            bs += l\n",
    "            \n",
    "    result = []\n",
    "    for s in X_dev:    \n",
    "        \n",
    "        len_dut = len(zlib.compress((ds + s).encode('utf-8'),9))\n",
    "        len_bel = len(zlib.compress((bs + s).encode('utf-8'),9))\n",
    "        \n",
    "        if len_bel > len_dut:\n",
    "            result.append(\"DUT\")\n",
    "        elif len_bel < len_dut:\n",
    "            result.append(\"BEL\")\n",
    "        else:\n",
    "            len_dut_a = len(zlib.compress((ds + ds + bs+ s).encode('utf-8'),9))\n",
    "            len_bel_a = len(zlib.compress((bs + bs + ds + s).encode('utf-8'),9))\n",
    "            if len_bel_a > len_dut_a:\n",
    "                result.append(\"DUT\")\n",
    "            elif len_bel_a < len_dut_a:\n",
    "                result.append(\"BEL\")\n",
    "            else:\n",
    "                score = random.randint(0,1)\n",
    "                result.append((\"DUT\", \"BEL\")[score > 0])\n",
    "    \n",
    "    update_stats(\"zlib\", y_dev, np.array(result), {})\n",
    "    \n",
    "summarise_run(\"zlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predictions: [[weight1, predictions1], ..., [weightn, predictionsn]]\n",
    "# DUT = -1.0, BEL = 1.0\n",
    "def ensemble(y_true, y_predictions):\n",
    "    for _, preds in y_predictions:\n",
    "        if len(preds) != len(y_true):\n",
    "            print(\"WTF\")\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        score = 0\n",
    "        #print(score)\n",
    "        for weight, preds in y_predictions:\n",
    "            score += weight * (-1.0, 1.0)[preds[i] == \"BEL\"]\n",
    "            #print(\"\\t\", score)\n",
    "        #print(score, (\"DUT\", \"BEL\")[score > 0], \"\\n\")\n",
    "        if score == 0.0:\n",
    "            score = random.randint(0,1)\n",
    "        scores.append((\"DUT\", \"BEL\")[score > 0])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ensemble([1,2,3,4], [[2.5,[\"DUT\", \"BEL\", \"BEL\", \"DUT\"]], [1.3, [\"BEL\", \"BEL\", \"DUT\", \"DUT\"]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  rfo_sc\n",
    "#  knn_st\n",
    "#  rfo_llh\n",
    "#  mnb_llh\n",
    "#  fat1\n",
    "#  mnb_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = \"DUT\"\n",
    "#(0, 1)[a == \"BEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# voting classifier\n",
    "def find_random_weights():\n",
    "    accs = []\n",
    "    best_acc = 0\n",
    "    for i in range(100000):\n",
    "        best_predictions = []\n",
    "        for method in _run_stats.keys():\n",
    "            best_predictions.append([random.randint(0,100), _run_stats[method]['best_predictions']])\n",
    "\n",
    "        acc = accuracy_score(y_dev, ensemble(y_dev, best_predictions))\n",
    "        accs.append([acc, [row[0] for row in best_predictions]])\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            weights = [row[0] for row in best_predictions]\n",
    "            norm_weights = [float(i)/sum(weights) for i in weights]\n",
    "            print(\"%.3f\" % acc, end=\"\\t\")\n",
    "            for w, n in list(zip(norm_weights, _run_stats.keys())):\n",
    "                print(\"%.2f %s\"  % (w, n), end=\"\\t\")\n",
    "            print()\n",
    "    return accs\n",
    "\n",
    "# majority baseline\n",
    "def find_uniform_weights():\n",
    "    accs = []\n",
    "    best_predictions = []\n",
    "    for method in _run_stats.keys():\n",
    "        best_predictions.append([1, _run_stats[method]['best_predictions']])\n",
    "\n",
    "    accs.append([accuracy_score(y_dev, ensemble(y_dev, best_predictions)), [row[0] for row in best_predictions]])\n",
    "    return accs\n",
    "\n",
    "print()\n",
    "\n",
    "from operator import itemgetter\n",
    "best_random_score = max(find_random_weights(),key=itemgetter(0))\n",
    "print(best_random_score)\n",
    "print(list(zip(best_random_score[1], _run_stats.keys())))\n",
    "\n",
    "from operator import itemgetter\n",
    "best_uniform_score = max(find_uniform_weights(),key=itemgetter(0))\n",
    "print(best_uniform_score)\n",
    "print(list(zip(best_uniform_score[1], _run_stats.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"DUT\", \"BEL\"])\n",
    "\n",
    "print(\"\\t\" + \"\\t\".join(_run_stats.keys()))\n",
    "for keya in _run_stats.keys():\n",
    "    a = le.transform(_run_stats[keya]['best_predictions'])\n",
    "    \n",
    "    print(keya, end=\"\\t\")\n",
    "    \n",
    "    for keyb in _run_stats.keys():\n",
    "        b = le.transform(_run_stats[keyb]['best_predictions'])\n",
    "        \n",
    "        print(\"%.4f\" % np.corrcoef(a,b)[0][1], end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONLY COMBINE WHEN THERE'S A LOW CORRELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#    for alpha in tqdm(10.0 ** -np.arange(1, 7), desc=\"alpha\"):\n",
    "#        for hls in tqdm([(5,2), (5,5), (10,2), (10,5), (50,2), (50,5), (50,10)], desc=\"hls\"):\n",
    "\n",
    "if do_run(\"mlp1\"):\n",
    "    mlp1 = TfidfVectorFeatures(\"mlp1\", \"c&w n-grams, tfidf, mlp adam\", MLPClassifier(solver='adam', alpha=alpha, hidden_layer_sizes=hls, random_state=1)).run()\n",
    "summarise_run(\"mlp1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "if do_run(\"mlp1\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"mlp1\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, xgboost\", \n",
    "                                               MLPClassifier(solver='adam', random_state=1), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"mlp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "if do_run(\"mlp1\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"mlp1\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, xgboost\", \n",
    "                                               MLPClassifier(solver='saga', random_state=1), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"mlp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if do_run(\"log_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"log_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, logit\", \n",
    "                                               SGDClassifier(loss=\"log\"), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"log_llh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_run(\"pct_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"pct_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, perceptron\", \n",
    "                                               SGDClassifier(loss=\"perceptron\"), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"pct_llh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "if do_run(\"bag1\"):\n",
    "    bag1 = TfidfVectorFeatures(\"bag1\", \"c&w n-grams, tfidf, bagging svc\", BaggingClassifier(LinearSVC(), n_estimators=10, max_samples=0.1)).run()\n",
    "summarise_run(\"bag1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#    for alpha in tqdm(10.0 ** -np.arange(1, 7), desc=\"alpha\"):\n",
    "#        for hls in tqdm([(5,2), (5,5), (10,2), (10,5), (50,2), (50,5), (50,10)], desc=\"hls\"):\n",
    "\n",
    "if do_run(\"mlp1\"):\n",
    "    mlp1 = TfidfVectorFeatures(\"mlp1\", \"c&w n-grams, tfidf, mlp adam\", MLPClassifier(solver='adam', alpha=alpha, hidden_layer_sizes=hls, random_state=1)).run()\n",
    "summarise_run(\"mlp1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select k-best character and word n-gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if do_run(\"rfo2\"):\n",
    "    \n",
    "    register_run(\"rfo1\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a random forest setting\",\n",
    "             ['count'])\n",
    " \n",
    "    print(\"Extracting features\")\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,6),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "    X_training_vec = vectorizer.fit_transform(X_training)\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_training_vect.shape)\n",
    "\n",
    "    print(\"Extracting from test data\")\n",
    "    X_dev_vec = vectorizer.transform(X_dev)\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_dev_vect.shape)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    for k in range(1,100000, 1000):\n",
    "\n",
    "\n",
    "        print(\"Extracting %d best features by a chi-squared test\" % k)\n",
    "        ch2 = SelectKBest(chi2, k=k)\n",
    "        #print(chi2(X_training_vect, y_training))\n",
    "        X_training_vect = ch2.fit_transform(X_training_vec, y_training)\n",
    "        X_dev_vect = ch2.transform(X_dev_vec)\n",
    "#         print(\"n_samples: %d, n_features: %d\" % X_training_vect.shape)\n",
    "#         print(\"n_samples: %d, n_features: %d\" % X_dev_vect.shape)\n",
    "\n",
    "\n",
    "        clf = RandomForestClassifier(max_depth=2)\n",
    "        clf.fit(X_training_vect, y_training)\n",
    "        print(k, clf.score(X_dev_vect, y_dev ))\n",
    "    \n",
    "#     for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "#         for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "\n",
    "#             for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "#                 for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "\n",
    "#                     steps = [('char', )),\n",
    "#                              ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "#                             ]\n",
    "\n",
    "#                     union = FeatureUnion(steps)\n",
    "\n",
    "#                     pipeline = Pipeline([\n",
    "#                         ('union', union),\n",
    "#                         ('rfo', RandomForestClassifier()),\n",
    "#                     ])\n",
    "\n",
    "#                     model = pipeline.fit(X_training, y_training)\n",
    "#                     print(model)\n",
    "\n",
    "#                     ch2 = SelectKBest(chi2, k=1000)\n",
    "#                     X_training_new = ch2.fit_transform(X_training, y_training)\n",
    "#                     X_dev_new = ch2.transform(X_dev)\n",
    "\n",
    "#                     y_pred = model.predict(X_dev)\n",
    "\n",
    "#                     update_stats(\"rfo2\", y_dev, y_pred, {'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n",
    "\n",
    "summarise_run(\"rfo2\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "if do_run(\"lgbm_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"lgbm_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, lightgbm\", \n",
    "                                               lgb.LGBMRegressor(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"lgbm_llh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp\n",
    "mlp is sensitive to feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if do_run(\"xgb1\"):\n",
    "    register_run(\"xgb1\",\n",
    "             \"character and word n-grams, with tf-idf feature vectors, used in a extreme gradient boost setting\",\n",
    "             [])\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "    parameters = {'nthread':[1], #when use hyperthread, xgboost may become slower\n",
    "                  'objective':['binary:logistic'],\n",
    "                  'learning_rate': [0.05], #so called `eta` value\n",
    "                  'max_depth': [6],\n",
    "                  'min_child_weight': [11],\n",
    "                  'silent': [1],\n",
    "                  'subsample': [0.8],\n",
    "                  'colsample_bytree': [0.7],\n",
    "                  'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "                  'missing':[-999],\n",
    "                  'seed': [1337]}\n",
    "\n",
    "    from sklearn.cross_validation import *\n",
    "    clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                       cv=StratifiedKFold(y_training[use], n_folds=5, shuffle=True), \n",
    "                       scoring='accuracy',\n",
    "                       verbose=2, refit=True)\n",
    "\n",
    "    #bst = clf.fit(X_training, y_training)\n",
    "    #xgb_model = xgb.XGBClassifier(nthread=1, objective='binary:logistic', learning_rate=0.05, max_depth=6, min_child_weight=11, missing=-999, n_estimators=1000, subsample=0.8, colsample_bytree=0.7)\n",
    "\n",
    "    #xgb.plot_importance(xgb_model)\n",
    "    #plt.show()\n",
    "    \n",
    "    #update_stats(\"xgb1\", y_dev, y_pred, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
