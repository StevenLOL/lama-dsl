{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucto\n",
    "import pickle\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can enter the parts that you want to run. The identifiers are the run names, and you can find them further\n",
    "# in the notebook. Use \"all\" to run all of them; this option overrules all other run names. Also note that it takes a\n",
    "# seriously large amount of time to run them all.\n",
    "\n",
    "#_run = [\"all\"]\n",
    "_run = []\n",
    "\n",
    "def do_run(name):\n",
    "    return name in _run or \"all\" in _run\n",
    "\n",
    "_run_max = {}\n",
    "\n",
    "def update_run(name, value, settings):\n",
    "    if _run_max.get(name, [0, \"\"]) < value:\n",
    "        _run_max[name] = [value, settings]\n",
    "        print(\"[%s]\\t%s\\t%s\" % (name, value, settings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(file):\n",
    "    with open(file) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucto_config = \"tokconfig-nld\"\n",
    "tokeniser = ucto.Tokenizer(ucto_config, sentenceperlineinput=True, sentencedetection=False, paragraphdetection=False)\n",
    "\n",
    "# We read the file with ucto and tokenise it according to its default Dutch tokenisation scheme, which is rule-based\n",
    "# and definitely better than a plain whitespace tokeniser from sklearn. Afterwards we concatenate the tokens back to a \n",
    "# whitespace seperated line, which can then be normally processed with sklearn's tokenisers.\n",
    "def read_data(file):\n",
    "    text = {}\n",
    "    with open(file) as f:\n",
    "        for line in tqdm(f):\n",
    "            sentence, language = line.strip().split(\"\\t\")\n",
    "            tokeniser.process(sentence)\n",
    "\n",
    "            if language not in text:\n",
    "                text[language] = []\n",
    "\n",
    "            current_line = []\n",
    "            for token in tokeniser:\n",
    "                current_line.append(str(token))\n",
    "                if token.isendofsentence():\n",
    "                    #print(current_line)\n",
    "                    text[language].append(\" \".join(current_line))\n",
    "                    current_line = []\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading development set from pickle.\n",
      "development set\n",
      "\t BEL \t 250 \t 40.456\n",
      "\t DUT \t 250 \t 40.088\n"
     ]
    }
   ],
   "source": [
    "# If this is the first run, then we have to tokenise the text. In other cases we probably have saved a pickled version\n",
    "# somewhere. If not, we will tokenise the text anyway. No worries.\n",
    "\n",
    "# First the development set\n",
    "try:\n",
    "    with open('data/dev.txt.pickle', 'rb') as f:\n",
    "        _l_dev_text = pickle.load(f)\n",
    "        print(\"Done reading development set from pickle.\")\n",
    "except IOError:\n",
    "    _l_dev_text = read_data('data/dev.txt')\n",
    "    print(\"Done tokenising development set.\")\n",
    "    with open('data/dev.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_dev_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing development set from pickle.\")\n",
    "\n",
    "print(\"development set\")\n",
    "print(\"\\tLAN\\tsize\\tavg length\")\n",
    "for l in _l_dev_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_dev_text[l]), \"\\t\", sum([len(x.split()) for x in _l_dev_text[l]])/len(_l_dev_text[l]))\n",
    "\n",
    "# And then the training set. This takes bit more time...\n",
    "try:\n",
    "    with open('data/train.txt.pickle', 'rb') as f:\n",
    "        _l_trn_text = pickle.load(f)\n",
    "        print(\"Done reading training set from pickle.\")\n",
    "except IOError:\n",
    "    _l_trn_text = read_data('data/train.txt')\n",
    "    print(\"Done tokenising training set.\")\n",
    "    with open('data/train.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_trn_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing training set from pickle.\")\n",
    "\n",
    "print(\"training set\")\n",
    "print(\"\\tLAN\\tsize\\tavg length\")\n",
    "for l in _l_trn_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_trn_text[l]), \"\\t\", sum([len(x.split()) for x in _l_trn_text[l]])/len(_l_trn_text[l]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading training set from pickle.\n",
      "training set\n",
      "\t BEL \t 150000 \t 40.273626666666665\n",
      "\t DUT \t 150000 \t 40.37152\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert the training and development material into the right shape, and make sure that we also keep track of\n",
    "# the labels.\n",
    "\n",
    "X_training = []\n",
    "y_training = []\n",
    "for l in _l_trn_text.keys():\n",
    "    for s in _l_trn_text[l]:\n",
    "        X_training.append(s)\n",
    "        y_training.append(l)\n",
    "X_training = np.array(X_training)\n",
    "y_training = np.array(y_training)\n",
    "\n",
    "\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "for l in _l_dev_text.keys():\n",
    "    for s in _l_dev_text[l]:\n",
    "        X_dev.append(s)\n",
    "        y_dev.append(l)\n",
    "X_dev = np.array(X_dev)\n",
    "y_dev = np.array(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes for testing whether some code words, you might want to use a subset. Use this one. Or another one. I don't\n",
    "# care. \n",
    "\n",
    "import random\n",
    "#use = random.sample(range(1, 299999), 100000)\n",
    "use = random.sample(range(1, 299999), 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# svc2: character and word n-grams, with count feature vectors, used in a linear support vector classifier setting\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#svc2_max = [0.578, '1, 2, 3, 5']\n",
    "if do_run(\"svc2\"):\n",
    "    \n",
    "    for min_cn in trange(1,8, desc=\"min char ngram\"):\n",
    "        for max_cn in trange(min_cn, 8, desc=\"max char ngram\"):\n",
    "            \n",
    "            for min_n in trange(1,6, desc=\"min word ngram\"):\n",
    "                for max_n in trange(min_n,6, desc=\"max word ngram\"):\n",
    "                    \n",
    "                    steps = [('char', CountVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                             ('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                            ]\n",
    "\n",
    "                    union = FeatureUnion(steps)\n",
    "\n",
    "                    pipeline = Pipeline([\n",
    "                        ('union', union),\n",
    "                        ('svc', SVC(kernel='linear')),\n",
    "                    ])\n",
    "\n",
    "                    prediction = pipeline.fit(X_training, y_training)\n",
    "                    score = prediction.score(X_dev, y_dev)\n",
    "\n",
    "                    update_run(\"svc2\", score, \"%s, %s, %s, %s\" % (min_cn, max_cn, min_n, max_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc1: character and word n-grams, with tf-idf feature vectors, used in a linear support vector classifier setting\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "svc1_max = [0.586, '1, 2, 4, 5']\n",
    "#svc1_max = [0, \"\"]\n",
    "\n",
    "if do_run(\"svc1\"):\n",
    "    \n",
    "    for min_cn in trange(1,8, desc=\"min char ngram\"):\n",
    "        for max_cn in trange(min_cn, 8, desc=\"max char ngram\"):\n",
    "            \n",
    "            for min_n in trange(1,6, desc=\"min word ngram\"):\n",
    "                for max_n in trange(min_n,6, desc=\"max word ngram\"):\n",
    "                    \n",
    "                    steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                             ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                            ]\n",
    "\n",
    "                    union = FeatureUnion(steps)\n",
    "\n",
    "                    pipeline = Pipeline([\n",
    "                        ('union', union),\n",
    "                        ('svc', SVC(kernel='linear')),\n",
    "                    ])\n",
    "\n",
    "                    prediction = pipeline.fit(X_training[use], y_training[use])\n",
    "                    score = prediction.score(X_dev, y_dev)\n",
    "\n",
    "                    update_run(\"svc1\", score, \"%s, %s, %s, %s\" % (min_cn, max_cn, min_n, max_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnb1: character and word n-grams, with tf-idf feature vectors, used in a multinominal naive bayes setting\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb1_max = [0.55, '4, 5, 5, 5']\n",
    "#mnb1_max = [0, \"\"]\n",
    "\n",
    "if do_run(\"mnb1\"):\n",
    "    \n",
    "    for min_cn in trange(1,8, desc=\"min char ngram\"):\n",
    "        for max_cn in trange(min_cn, 8, desc=\"max char ngram\"):\n",
    "            \n",
    "            for min_n in trange(1,6, desc=\"min word ngram\"):\n",
    "                for max_n in trange(min_n,6, desc=\"max word ngram\"):\n",
    "                    \n",
    "                    steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                             ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                            ]\n",
    "\n",
    "                    union = FeatureUnion(steps)\n",
    "\n",
    "                    pipeline = Pipeline([\n",
    "                        ('union', union),\n",
    "                        ('mnb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "                    prediction = pipeline.fit(X_training[use], y_training[use])\n",
    "                    score = prediction.score(X_dev, y_dev)\n",
    "\n",
    "                    update_score(\"mnb1\", score, \"%s, %s, %s, %s\" % (min_cn, max_cn, min_n, max_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn1: character and word n-grams, with tf-idf feature vectors, used in a k nearest neighbours setting\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn1_max = [0.562, '4, 3, 7, 5, 5']\n",
    "\n",
    "if do_run(\"knn1\"):\n",
    "    \n",
    "    for neighbours in trange(1,7, desc=\"neighbours\"):\n",
    "\n",
    "        for min_cn in trange(1,8, desc=\"min char ngram\"):\n",
    "            for max_cn in trange(min_cn, 8, desc=\"max char ngram\"):\n",
    "\n",
    "                for min_n in trange(1,6, desc=\"min word ngram\"):\n",
    "                    for max_n in trange(min_n,6, desc=\"max word ngram\"):\n",
    "\n",
    "                        steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                                 ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                                ]\n",
    "\n",
    "                        union = FeatureUnion(steps)\n",
    "\n",
    "                        pipeline = Pipeline([\n",
    "                            ('union', union),\n",
    "                            ('mnb', KNeighborsClassifier(n_neighbors=neighbours)),\n",
    "                        ])\n",
    "\n",
    "                        prediction = pipeline.fit(X_training[use], y_training[use])\n",
    "                        score = prediction.score(X_dev, y_dev)\n",
    "\n",
    "                        update_run(\"knn1\", score, \"%s, %s, %s, %s, %s\" % (neighbours, min_cn, max_cn, min_n, max_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftt1: character and word n-grams, with embeddings, used in a fasttext (w2v sg) setting\n",
    "\n",
    "import fasttext\n",
    "\n",
    "if do_run(\"fat1\"):\n",
    "\n",
    "    with open('fasttext.train.txt', 'w') as f:\n",
    "        for line, label in zip(X_training[use], y_training[use]):\n",
    "            f.write(line + \" __language__\" + label + \"\\n\")\n",
    "\n",
    "    ft_classifier = fasttext.supervised('fasttext.train.txt', 'model', \n",
    "                                        min_count=1, \n",
    "                                        word_ngrams=3, \n",
    "                                        minn=7, \n",
    "                                        maxn=7, \n",
    "                                        thread=2, \n",
    "                                        label_prefix='__language__')\n",
    "    ft_predictions = ft_classifier.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lm \n",
    "which patterns are more important compared to the \"all\" background corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LLHbasedBinaryVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, count=1000):\n",
    "        self.llh_1000 = []\n",
    "        with open('data/DUT_BEL.t2m1l3.llh.top1000', 'r') as f:\n",
    "            for n, line in enumerate(f):\n",
    "                self.llh_1000.append(line.split(\"\\t\")[0])\n",
    "                if n >= count:\n",
    "                    break\n",
    "    \n",
    "    def llh_binary_countvectorizer(self, line):\n",
    "        values = []\n",
    "        for k in self.llh_1000:\n",
    "            values.append(1*(k in line))\n",
    "        return values\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        result = []\n",
    "        for l in df:\n",
    "            result.append(self.llh_binary_countvectorizer(l))\n",
    "        return result\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "svc3_max = [0, \"\"]\n",
    "\n",
    "if do_run(\"svc3\"):\n",
    "    for count in tqdm(range(1, 10000, 500), desc=\"# llh counts\"):\n",
    "\n",
    "        steps = [('llh', LLHbasedBinaryVectorizer(count=count)),\n",
    "                 #('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                ]\n",
    "\n",
    "        union = FeatureUnion(steps)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('union', union),\n",
    "            ('svc', SVC(kernel='linear')),\n",
    "        ])\n",
    "\n",
    "        prediction = pipeline.fit(X_training[use], y_training[use])\n",
    "        score = prediction.score(X_dev, y_dev)\n",
    "\n",
    "        update_run(\"svc3\", score, str(count))\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp\n",
    "mlp is sensitive to feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp1: character and word n-grams, with tf-idf feature vectors, used in a multilayer perceptron (adam) setting\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp1_max = [0, \"\"]\n",
    "\n",
    "if do_run(\"mlp1\"):\n",
    "    for alpha in tqdm(10.0 ** -np.arange(1, 7), desc=\"alpha\"):\n",
    "        for hls in tqdm([(5,2), (5,5), (10,2), (10,5), (50,2), (50,5), (50,10)], desc=\"hls\"):\n",
    "\n",
    "            for min_cn in trange(1,8, desc=\"min char ngram\"):\n",
    "                for max_cn in trange(min_cn, 8, desc=\"max char ngram\"):\n",
    "\n",
    "                    for min_n in trange(1,6, desc=\"min word ngram\"):\n",
    "                        for max_n in trange(min_n,6, desc=\"max word ngram\"):\n",
    "                            \n",
    "                            steps = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                                     ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                                    ]\n",
    "\n",
    "                            union = FeatureUnion(steps)\n",
    "\n",
    "                            pipeline = Pipeline([\n",
    "                                ('union', union),\n",
    "                                ('mlp1', MLPClassifier(solver='adam', alpha=alpha, hidden_layer_sizes=hls, random_state=1)),\n",
    "                            ])\n",
    "\n",
    "                            prediction = pipeline.fit(X_training[use], y_training[use])\n",
    "                            score = prediction.score(X_dev, y_dev)\n",
    "\n",
    "                            update_run(\"mlp1\", score, \"%s, %s, %s, %s, %s, %s\" % (alpha, hls, min_cn, max_cn, min_n, max_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb1: character and word n-grams, with tf-idf feature vectors, used in a extreme gradient boost setting\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "if do_run(\"xgb1\"):\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "    parameters = {'nthread':[1], #when use hyperthread, xgboost may become slower\n",
    "                  'objective':['binary:logistic'],\n",
    "                  'learning_rate': [0.05], #so called `eta` value\n",
    "                  'max_depth': [6],\n",
    "                  'min_child_weight': [11],\n",
    "                  'silent': [1],\n",
    "                  'subsample': [0.8],\n",
    "                  'colsample_bytree': [0.7],\n",
    "                  'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "                  'missing':[-999],\n",
    "                  'seed': [1337]}\n",
    "\n",
    "    from sklearn.cross_validation import *\n",
    "    clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                       cv=StratifiedKFold(y_training[use], n_folds=5, shuffle=True), \n",
    "                       scoring='accuracy',\n",
    "                       verbose=2, refit=True)\n",
    "\n",
    "    bst = clf.fit(X_training[use], y_training[use])\n",
    "    xgb_model = xgb.XGBClassifier(nthread=1, objective='binary:logistic', learning_rate=0.05, max_depth=6, min_child_weight=11, missing=-999, n_estimators=1000, subsample=0.8, colsample_bytree=0.7)\n",
    "\n",
    "    xgb.plot_importance(xgb_model)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
