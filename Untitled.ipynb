{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ucto\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can enter the parts that you want to run. The identifiers are the run names, and you can find them further\n",
    "# in the notebook. Use \"all\" to run all of them; this option overrules all other run names. Also note that it takes a\n",
    "# seriously large amount of time to run them all.\n",
    "\n",
    "_run = set([])\n",
    "_use_subset = False\n",
    "_show_graphics = True\n",
    "\n",
    "import sys\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # running in console\n",
    "    _show_graphics = False\n",
    "    from tqdm import tqdm as tqdm, trange as tnrange\n",
    "    \n",
    "    for v in sys.argv:\n",
    "        if v.startswith(\"r:\"):\n",
    "            _run.add(v[2:])\n",
    "        elif v == \"o:subset\":\n",
    "            _use_subset = True\n",
    "else:\n",
    "    # running in notebook\n",
    "    from tqdm import tqdm_notebook as tqdm, tnrange as tnrange\n",
    "    \n",
    "    _run.add(\"all\")\n",
    "    \n",
    "    _use_subset = True\n",
    "    pass\n",
    "\n",
    "def do_run(name):\n",
    "    return name in _run or \"all\" in _run\n",
    "\n",
    "_run_stats = {}\n",
    "\n",
    "def register_run(name, description, params):\n",
    "    if name not in _run_stats:\n",
    "        _run_stats[name] = {}\n",
    "        _run_stats[name]['description'] = description\n",
    "        _run_stats[name]['params'] = {}\n",
    "        for param in params:\n",
    "            _run_stats[name]['params'][param] = ''\n",
    "        \n",
    "        _run_stats[name]['best'] = 0\n",
    "        _run_stats[name]['best_predictions'] = []\n",
    "        _run_stats[name]['orig_predictions'] = []\n",
    "        _run_stats[name]['history'] = []\n",
    "\n",
    "def update_stats(name, y_test, y_pred, params):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    _run_stats[name]['history'].append(accuracy)\n",
    "    if accuracy > _run_stats[name]['best']:\n",
    "        _run_stats[name]['best'] = accuracy\n",
    "        for param in params:\n",
    "            _run_stats[name]['params'][param] = params[param]\n",
    "        _run_stats[name]['best_predictions'] = y_pred\n",
    "        _run_stats[name]['orig_predictions'] = y_test\n",
    "        \n",
    "def summarise_run(name):\n",
    "    if name not in _run_stats:\n",
    "        print(\"Name %s is not registrered.\" % (name))\n",
    "        return\n",
    "    \n",
    "    print(\"Model:    \\n\\t%s\" % (name))\n",
    "    print(\"Settings: \\n\\t%s\" % (_run_stats[name]['params']))\n",
    "    print(\"Accuracy: \\n\\t%s\" % (_run_stats[name]['best']))\n",
    "    \n",
    "    cm = confusion_matrix(_run_stats[name]['orig_predictions'], _run_stats[name]['best_predictions'])\n",
    "    \n",
    "    if _show_graphics:\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm, classes=[\"BEL\", \"DUT\"], title=name + \" \" + str(_run_stats[name]['params']))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(cm)       \n",
    "\n",
    "def summarise_all():\n",
    "    for name in _run_stats.keys():\n",
    "        summarise_run(name)\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(file):\n",
    "    with open(file) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucto_config = \"tokconfig-nld\"\n",
    "tokeniser = ucto.Tokenizer(ucto_config, sentenceperlineinput=True, sentencedetection=False, paragraphdetection=False)\n",
    "\n",
    "# We read the file with ucto and tokenise it according to its default Dutch tokenisation scheme, which is rule-based\n",
    "# and definitely better than a plain whitespace tokeniser from sklearn. Afterwards we concatenate the tokens back to a \n",
    "# whitespace seperated line, which can then be normally processed with sklearn's tokenisers.\n",
    "def read_data(file):\n",
    "    text = {}\n",
    "    with open(file) as f:\n",
    "        for line in tqdm(f):\n",
    "            sentence, language = line.strip().split(\"\\t\")\n",
    "            tokeniser.process(sentence)\n",
    "\n",
    "            if language not in text:\n",
    "                text[language] = []\n",
    "\n",
    "            current_line = []\n",
    "            for token in tokeniser:\n",
    "                current_line.append(str(token))\n",
    "                if token.isendofsentence():\n",
    "                    #print(current_line)\n",
    "                    text[language].append(\" \".join(current_line))\n",
    "                    current_line = []\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading development set from pickle.\n",
      "development set\n",
      "\t LAN\t size \t avg length\n",
      "\t BEL \t 250 \t 40.456\n",
      "\t DUT \t 250 \t 40.088\n",
      "Done reading training set from pickle.\n",
      "training set\n",
      "\t LAN\t size \t avg length\n",
      "\t BEL \t 150000 \t 40.273626666666665\n",
      "\t DUT \t 150000 \t 40.37152\n"
     ]
    }
   ],
   "source": [
    "# If this is the first run, then we have to tokenise the text. In other cases we probably have saved a pickled version\n",
    "# somewhere. If not, we will tokenise the text anyway. No worries.\n",
    "\n",
    "# First the development set\n",
    "try:\n",
    "    with open('data/dev.txt.pickle', 'rb') as f:\n",
    "        _l_dev_text = pickle.load(f)\n",
    "        print(\"Done reading development set from pickle.\")\n",
    "except IOError:\n",
    "    _l_dev_text = read_data('data/dev.txt')\n",
    "    print(\"Done tokenising development set.\")\n",
    "    with open('data/dev.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_dev_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing development set from pickle.\")\n",
    "\n",
    "print(\"development set\")\n",
    "print(\"\\t LAN\\t size \\t avg length\")\n",
    "for l in _l_dev_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_dev_text[l]), \"\\t\", sum([len(x.split()) for x in _l_dev_text[l]])/len(_l_dev_text[l]))\n",
    "\n",
    "# And then the training set. This takes bit more time...\n",
    "try:\n",
    "    with open('data/train.txt.pickle', 'rb') as f:\n",
    "        _l_trn_text = pickle.load(f)\n",
    "        print(\"Done reading training set from pickle.\")\n",
    "except IOError:\n",
    "    _l_trn_text = read_data('data/train.txt')\n",
    "    print(\"Done tokenising training set.\")\n",
    "    with open('data/train.txt.pickle', 'wb') as f:\n",
    "        pickle.dump(_l_trn_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done writing training set from pickle.\")\n",
    "\n",
    "print(\"training set\")\n",
    "print(\"\\t LAN\\t size \\t avg length\")\n",
    "for l in _l_trn_text.keys():\n",
    "    print(\"\\t\", l, \"\\t\", len(_l_trn_text[l]), \"\\t\", sum([len(x.split()) for x in _l_trn_text[l]])/len(_l_trn_text[l]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert the training and development material into the right shape, and make sure that we also keep track of\n",
    "# the labels.\n",
    "\n",
    "X_training = []\n",
    "y_training = []\n",
    "for l in _l_trn_text.keys():\n",
    "    for s in _l_trn_text[l]:\n",
    "        X_training.append(s)\n",
    "        y_training.append(l)\n",
    "X_training = np.array(X_training)\n",
    "y_training = np.array(y_training)\n",
    "\n",
    "\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "for l in _l_dev_text.keys():\n",
    "    for s in _l_dev_text[l]:\n",
    "        X_dev.append(s)\n",
    "        y_dev.append(l)\n",
    "X_dev = np.array(X_dev)\n",
    "y_dev = np.array(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training subset\n"
     ]
    }
   ],
   "source": [
    "# Sometimes for testing whether some code words, you might want to use a subset. Use this one. Or another one. I don't\n",
    "# care. \n",
    "\n",
    "import random\n",
    "#use = random.sample(range(1, 299999), 100000)\n",
    "use = random.sample(range(1, 299999), 10000)\n",
    "\n",
    "if _use_subset:   \n",
    "    X_training = X_training[use]\n",
    "    y_training = y_training[use]\n",
    "    \n",
    "    print(\"training subset\")\n",
    "    #print(\"\\t LAN\\t size \\t avg length\")\n",
    "    #for l in _l_trn_text.keys():\n",
    "    #    print(\"\\t\", l, \"\\t\", len(_l_trn_text[l]), \"\\t\", sum([len(x.split()) for x in _l_trn_text[l]])/len(_l_trn_text[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerFeatures():\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, features, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6):\n",
    "        self.run_name = name\n",
    "        self.run_description = description\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        self.features = features\n",
    "        self.feature_selection = feature_selection\n",
    "        self.scaler = scaler\n",
    "    \n",
    "        self.run_min_cn = min_cn\n",
    "        self.run_max_cn = max_cn\n",
    "        \n",
    "        self.run_min_n = min_n\n",
    "        self.run_max_n = max_n\n",
    "    \n",
    "        register_run(self.run_name, self.run_description, [])\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        #print(\"Classifier:\", self.classifier)\n",
    "        #print(\"Feature extraction:\", self.features)\n",
    "        #print(\"Feature selection:\", self.feature_selection)\n",
    "        #print(\"Feature scaler:\", self.scaler)\n",
    "                \n",
    "        self.pipeline = Pipeline([\n",
    "            ('features',   FeatureUnion(self.features)),\n",
    "            ('scaler',     self.scaler),\n",
    "            ('selection',  self.feature_selection),\n",
    "            ('classifier', self.classifier),\n",
    "        ])\n",
    "\n",
    "        model = self.pipeline.fit(X_training, y_training)\n",
    "        self.y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(self.run_name, y_dev, self.y_pred, {'min_cn': self.run_min_cn, 'max_cn': self.run_max_cn, 'min_n': self.run_min_n, 'max_n': self.run_max_n})\n",
    "        \n",
    "        return accuracy_score(y_dev, self.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "class CountVectorFeatures(VectorizerFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6):\n",
    "        \n",
    "        features = [('char', CountVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(CountVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, features, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "        \n",
    "class SelectCountVectorFeatures(CountVectorFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6, k = 100):\n",
    "        features = [('char', CountVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', CountVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(SelectCountVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "        self.k = k\n",
    "        self.feature_selection = SelectKBest(chi2, k=k)\n",
    "\n",
    "##\n",
    "        \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class TfidfVectorFeatures(VectorizerFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6):\n",
    "        \n",
    "        features = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(TfidfVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, features, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "        \n",
    "class SelectTfidfVectorFeatures(TfidfVectorFeatures):\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, min_cn = 1, max_cn = 8, min_n = 1, max_n = 6, k = 100):\n",
    "        features = [('char', TfidfVectorizer(analyzer='char', ngram_range=(min_cn,max_cn))),\n",
    "                    ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        super(SelectTfidfVectorFeatures, self).__init__(name, X, Y, x, y, description, classifier, feature_selection, scaler, min_cn, max_cn, min_n, max_n)\n",
    "\n",
    "        self.k = k\n",
    "        self.feature_selection = SelectKBest(chi2, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "from abc import ABCMeta\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "\n",
    "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.svm_ = [] # fuggly\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # so we don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "\n",
    "        # Count raw events from data\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
    "                                 dtype=np.float64)\n",
    "        self._compute_ratios(X, Y)\n",
    "\n",
    "        # flugglyness\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
    "            Y_i = Y[:,i]\n",
    "            svm.fit(X_i, Y_i)\n",
    "            self.svm_.append(svm) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_effective_classes = self.class_count_.shape[0]\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        D = np.zeros((n_effective_classes, n_examples))\n",
    "\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            D[i] = self.svm_[i].decision_function(X_i)\n",
    "        \n",
    "        return self.classes_[np.argmax(D, axis=0)]\n",
    "        \n",
    "    def _compute_ratios(self, X, Y):\n",
    "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "\n",
    "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
    "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
    "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
    "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
    "        check_array(self.ratios_)\n",
    "        self.ratios_ = sparse.csr_matrix(self.ratios_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# 100 0.658\n",
    "# 1000 0.658\n",
    "# 10000 0.648\n",
    "# 100000 0.656\n",
    "# Model:    \n",
    "#         rfo_sc\n",
    "# Settings: \n",
    "#         {'max_n': 6, 'max_cn': 8, 'min_cn': 1, 'min_n': 1}\n",
    "# Accuracy: \n",
    "#         0.658\n",
    "# [[190  60]\n",
    "#  [111 139]]\n",
    "if do_run(\"rfo_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"rfo_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, counts, random forests\", \n",
    "                                            RandomForestClassifier(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"rfo_sc\")\n",
    "\n",
    "# 100 0.572\n",
    "# 1000 0.558\n",
    "# 10000 0.572\n",
    "# 100000 0.586\n",
    "# Model:    \n",
    "#         mnb_sc\n",
    "# Settings: \n",
    "#         {'min_n': 1, 'max_n': 6, 'min_cn': 1, 'max_cn': 8}\n",
    "# Accuracy: \n",
    "#         0.586\n",
    "# [[154  96]\n",
    "#  [111 139]]\n",
    "if do_run(\"mnb_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"mnb_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, counts, mult naive bayes\", \n",
    "                                            MultinomialNB(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"mnb_sc\") \n",
    "\n",
    "# 100 0.5\n",
    "# 1000 0.57\n",
    "# 10000 0.558\n",
    "# 100000 0.628\n",
    "# Model:    \n",
    "#         sgd_sc\n",
    "# Settings: \n",
    "#         {'max_n': 6, 'max_cn': 8, 'min_cn': 1, 'min_n': 1}\n",
    "# Accuracy: \n",
    "#         0.628\n",
    "# [[175  75]\n",
    "#  [111 139]]\n",
    "if do_run(\"knn_sc\"):\n",
    "    for n in range(1,7,2):\n",
    "        for k in [100, 1000, 10000, 100000]:\n",
    "             print(n, k, SelectCountVectorFeatures(\"knn_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, counts, knn\", \n",
    "                                                   KNeighborsClassifier(n_neighbors=n), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"knn_sc\") \n",
    "\n",
    "if do_run(\"sgd_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectCountVectorFeatures(\"sgd_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, counts, sgd\", \n",
    "                                               SGDClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"sgd_sc\")\n",
    "\n",
    "if do_run(\"nbs_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1]:\n",
    "             print(k, C, SelectCountVectorFeatures(\"nbs_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, counts, naive bayes/svm\", \n",
    "                                                   NBSVM(C=C), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"nbs_sc\")\n",
    "\n",
    "# 100 0.602\n",
    "# 1000 0.6\n",
    "# 10000 0.594\n",
    "# 100000 0.592\n",
    "# Model:    \n",
    "#         xgb_sc\n",
    "# Settings: \n",
    "#         {'min_cn': 1, 'max_cn': 8, 'min_n': 1, 'max_n': 6}\n",
    "# Accuracy: \n",
    "#         0.602\n",
    "# [[169  81]\n",
    "#  [118 132]]\n",
    "if do_run(\"xgb_sc\"):\n",
    "    for k in [100000]:\n",
    "        for lr in [0.05]:\n",
    "            for mcw in [1]:\n",
    "                for alpha in [0]:\n",
    "                    print(k, lr, mcw, alpha, SelectCountVectorFeatures(\"xgb_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                                           \"c&w n-grams, counts, xgboost\", \n",
    "                                                           xgb.XGBClassifier(silent=0, learning_rate=lr, min_child_weight=mcw, reg_alpha=alpha), \n",
    "                                                           k=k).run())\n",
    "summarise_run(\"xgb_sc\")\n",
    "\n",
    "if do_run(\"lsc_sc\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1, 0.5, 1.0]:\n",
    "             print(k, C, SelectCountVectorFeatures(\"lsc_sc\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, count, linear svc\", \n",
    "                                                   LinearSVC(C=C), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"lsc_sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# 100 0.628\n",
    "# 1000 0.644\n",
    "# 10000 0.638\n",
    "# 100000 0.63\n",
    "# Model:    \n",
    "#         rfo_st\n",
    "# Settings: \n",
    "#         {'min_n': 1, 'max_n': 6, 'min_cn': 1, 'max_cn': 8}\n",
    "# Accuracy: \n",
    "#         0.644\n",
    "# [[181  69]\n",
    "#  [109 141]]\n",
    "if do_run(\"rfo_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectTfidfVectorFeatures(\"rfo_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, tfidf, random forests\", \n",
    "                                            RandomForestClassifier(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"rfo_st\")\n",
    "\n",
    "# 100 0.546\n",
    "# 1000 0.566\n",
    "# 10000 0.596\n",
    "# 100000 0.616\n",
    "# Model:    \n",
    "#         mnb_st\n",
    "# Settings: \n",
    "#         {'min_n': 1, 'min_cn': 1, 'max_n': 6, 'max_cn': 8}\n",
    "# Accuracy: \n",
    "#         0.616\n",
    "# [[163  87]\n",
    "#  [105 145]]\n",
    "if do_run(\"mnb_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectTfidfVectorFeatures(\"mnb_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, tfidf, mult naive bayes\", \n",
    "                                            MultinomialNB(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"mnb_st\") \n",
    "\n",
    "if do_run(\"knn_st\"):\n",
    "    for n in range(1,7,2):\n",
    "        for k in [100, 1000, 10000, 100000]:\n",
    "             print(n, k, SelectTfidfVectorFeatures(\"knn_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, tfidf, knn\", \n",
    "                                                   KNeighborsClassifier(n_neighbors=n), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"knn_st\") \n",
    "\n",
    "# 100 0.568\n",
    "# 1000 0.57\n",
    "# 10000 0.572\n",
    "# 100000 0.58\n",
    "# Model:    \n",
    "#         sgd_st\n",
    "# Settings: \n",
    "#         {'max_n': 6, 'min_n': 1, 'max_cn': 8, 'min_cn': 1}\n",
    "# Accuracy: \n",
    "#         0.58\n",
    "# [[140 110]\n",
    "#  [100 150]]\n",
    "if do_run(\"sgd_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectTfidfVectorFeatures(\"sgd_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, tfidf, sgd\", \n",
    "                                               SGDClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"sgd_st\")\n",
    "\n",
    "# 100 0.598\n",
    "# 1000 0.584\n",
    "# 10000 0.594\n",
    "# 100000 0.588\n",
    "# Model:    \n",
    "#         xgb_st\n",
    "# Settings: \n",
    "#         {'max_cn': 8, 'max_n': 6, 'min_n': 1, 'min_cn': 1}\n",
    "# Accuracy: \n",
    "#         0.598\n",
    "# [[163  87]\n",
    "#  [114 136]]\n",
    "if do_run(\"xgb_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, SelectTfidfVectorFeatures(\"xgb_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, tfidf, xgboost\", \n",
    "                                               xgb.XGBClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"xgb_st\")\n",
    "\n",
    "\n",
    "# 100 0.1 0.582\n",
    "# 1000 0.1 0.584\n",
    "# 10000 0.1 0.61\n",
    "# 100000 0.1 0.608\n",
    "# Model:    \n",
    "#         nbs_st\n",
    "# Settings: \n",
    "#         {'min_n': 1, 'max_cn': 8, 'max_n': 6, 'min_cn': 1}\n",
    "# Accuracy: \n",
    "#         0.61\n",
    "# [[133 117]\n",
    "#  [ 78 172]]\n",
    "if do_run(\"nbs_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1]:\n",
    "             print(k, C, SelectTfidfVectorFeatures(\"nbs_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, tfidf, naive bayes/svm\", \n",
    "                                                   NBSVM(C=C), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"nbs_st\")\n",
    "\n",
    "if do_run(\"xgb_st1\"):\n",
    "    for k in [100000]:\n",
    "        for lr in [0.05]:\n",
    "            for mcw in [1]:\n",
    "                for alpha in [0]:\n",
    "                    print(k, lr, mcw, alpha, SelectTfidfVectorFeatures(\"xgb_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                                           \"c&w n-grams, tfidf, xgboost\", \n",
    "                                                           xgb.XGBClassifier(silent=0, learning_rate=lr, min_child_weight=mcw, reg_alpha=alpha), \n",
    "                                                           k=k).run())\n",
    "summarise_run(\"xgb_st\")\n",
    "\n",
    "if do_run(\"lsc_st\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1, 0.5, 1.0]:\n",
    "             print(k, C, SelectTfidfVectorFeatures(\"lsc_st\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, tfidf, linear svc\", \n",
    "                                                   LinearSVC(C=C), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"lsc_st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loglikelihood-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a new vectoriser based on the loglikelihood values as computed by colibricore-loglikelihood.\n",
    "# Based on some thresholds (n-gram occurrence >= 2, 1 <= n <= 3 -grams), it computes the occurrence counts,\n",
    "# their frequency and the corresponding loglikelihood scores.\n",
    "# These scores are sorted, and then this vectoriser takes the top m patterns, and marks whether that pattern\n",
    "# is presented in the given set.\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LLHbasedBinaryVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, count=1000):\n",
    "        self.llh_1000 = []\n",
    "        with open('data/DUT_BEL.t5m1l6.llh_t', 'r') as f:\n",
    "            for n, line in enumerate(f):\n",
    "                self.llh_1000.append(line.split(\"\\t\")[0])\n",
    "                if n >= count:\n",
    "                    break\n",
    "    \n",
    "    def llh_binary_countvectorizer(self, line):\n",
    "        values = []\n",
    "        for k in self.llh_1000:\n",
    "            values.append(1*(k in line))\n",
    "        return values\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        result = []\n",
    "        for l in df:\n",
    "            result.append(self.llh_binary_countvectorizer(l))\n",
    "        return result\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLHbasedFeatures():\n",
    "    \n",
    "    def __init__(self, name, X, Y, x, y, description, classifier, feature_selection = None, scaler = None, k=1000):\n",
    "        self.run_name = name\n",
    "        self.run_description = description\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        self.features = [('llh', LLHbasedBinaryVectorizer(count=k)),\n",
    "                    #('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "                   ]\n",
    "        self.feature_selection = feature_selection\n",
    "        self.scaler = scaler\n",
    "    \n",
    "        self.k = k\n",
    "    \n",
    "        register_run(self.run_name, self.run_description, [])\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        #print(\"Classifier:\", self.classifier)\n",
    "        #print(\"Feature extraction:\", self.features)\n",
    "        #print(\"Feature selection:\", self.feature_selection)\n",
    "        #print(\"Feature scaler:\", self.scaler)\n",
    "                \n",
    "        self.pipeline = Pipeline([\n",
    "            ('features',   FeatureUnion(self.features)),\n",
    "            ('scaler',     self.scaler),\n",
    "            ('selection',  self.feature_selection),\n",
    "            ('classifier', self.classifier),\n",
    "        ])\n",
    "\n",
    "        model = self.pipeline.fit(X_training, y_training)\n",
    "        self.y_pred = model.predict(X_dev)\n",
    "\n",
    "        update_stats(self.run_name, y_dev, self.y_pred, {'k': self.k})\n",
    "        \n",
    "        return accuracy_score(y_dev, self.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "if do_run(\"rfo_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"rfo_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, llh, random forests\", \n",
    "                                            RandomForestClassifier(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"rfo_llh\")\n",
    "\n",
    "if do_run(\"mnb_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"mnb_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                            \"c&w n-grams, llh, mult naive bayes\", \n",
    "                                            MultinomialNB(), \n",
    "                                            k=k).run())\n",
    "summarise_run(\"mnb_llh\") \n",
    "\n",
    "if do_run(\"knn_llh\"):\n",
    "    for n in range(1,7,2):\n",
    "        for k in [100, 1000, 10000, 100000]:\n",
    "             print(n, k, LLHbasedFeatures(\"knn_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, llh, knn\", \n",
    "                                                   KNeighborsClassifier(n_neighbors=n), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"knn_llh\") \n",
    "\n",
    "if do_run(\"sgd_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"sgd_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, sgd\", \n",
    "                                               SGDClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"sgd_llh\")\n",
    "\n",
    "if do_run(\"log_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"log_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, logit\", \n",
    "                                               SGDClassifier(loss=\"log\"), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"log_llh\")\n",
    "\n",
    "if do_run(\"pct_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"pct_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, perceptron\", \n",
    "                                               SGDClassifier(loss=\"perceptron\"), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"pct_llh\")\n",
    "\n",
    "if do_run(\"xgb_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"xgb_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, xgboost\", \n",
    "                                               xgb.XGBClassifier(), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"xgb_llh\")\n",
    "\n",
    "if do_run(\"nbs_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1]:\n",
    "             print(k, C, LLHbasedFeatures(\"nbs_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, llh, naive bayes/svm\", \n",
    "                                                   NBSVM(C=C), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"nbs_llh\")\n",
    "\n",
    "if do_run(\"xgb_llh\"):\n",
    "    for k in [100000]:\n",
    "        for lr in [0.05]:\n",
    "            for mcw in [1]:\n",
    "                for alpha in [0]:\n",
    "                    print(k, lr, mcw, alpha, LLHbasedFeatures(\"xgb_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                                           \"c&w n-grams, llh, xgboost\", \n",
    "                                                           xgb.XGBClassifier(silent=0, learning_rate=lr, min_child_weight=mcw, reg_alpha=alpha), \n",
    "                                                           k=k).run())\n",
    "summarise_run(\"xgb_llh\")\n",
    "\n",
    "if do_run(\"lsc_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "        for C in [0.1, 0.5, 1.0]:\n",
    "             print(k, C, LLHbasedFeatures(\"lsc_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                                   \"c&w n-grams, llh, linear svc\", \n",
    "                                                   LinearSVC(C=C), \n",
    "                                                   k=k).run())\n",
    "summarise_run(\"lsc_llh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Model:    \n",
    "#         fat1\n",
    "# Settings: \n",
    "#         {}\n",
    "# Accuracy: \n",
    "#         0.692\n",
    "# [[162  88]\n",
    "#  [ 66 184]]\n",
    "if do_run(\"fat1\"):\n",
    "    \n",
    "    register_run(\"fat1\",\n",
    "             \"character and word n-grams, with embeddings, used in a fasttext (w2v sg) setting\",\n",
    "             [])\n",
    "\n",
    "    with open('fasttext.train.txt', 'w') as f:\n",
    "        for line, label in zip(X_training, y_training):\n",
    "            f.write(line + \" __language__\" + label + \"\\n\")\n",
    "\n",
    "    ft_classifier = fasttext.supervised('fasttext.train.txt', 'model', \n",
    "                                        min_count=1, \n",
    "                                        word_ngrams=5, \n",
    "                                        epoch=100,\n",
    "                                        minn=3, \n",
    "                                        maxn=7, \n",
    "                                        thread=2, \n",
    "                                        dim=250,\n",
    "                                        ws=5,\n",
    "                                        bucket= 2000000,\n",
    "                                        label_prefix='__language__')\n",
    "    ft_predictions = ft_classifier.predict(X_dev)\n",
    "    \n",
    "    update_stats(\"fat1\", y_dev, ft_predictions, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predictions: [[weight1, predictions1], ..., [weightn, predictionsn]]\n",
    "def ensemble(y_true, y_predictions):\n",
    "    for _, preds in y_predictions:\n",
    "        if len(preds) != len(y_true):\n",
    "            print(\"WTF\")\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        score = 0\n",
    "        for weight, preds in y_predictions:\n",
    "            score += weight * preds[i]\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 66, 58, 50]\n"
     ]
    }
   ],
   "source": [
    "print(ensemble([1,2,3,4], [[1,[2,3,4,5]], [9, [8,7,6,5]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len([1,2,3,4])):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7b2b464e5261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hoi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "if not a[5]:\n",
    "    print(\"hoi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#    for alpha in tqdm(10.0 ** -np.arange(1, 7), desc=\"alpha\"):\n",
    "#        for hls in tqdm([(5,2), (5,5), (10,2), (10,5), (50,2), (50,5), (50,10)], desc=\"hls\"):\n",
    "\n",
    "if do_run(\"mlp1\"):\n",
    "    mlp1 = TfidfVectorFeatures(\"mlp1\", \"c&w n-grams, tfidf, mlp adam\", MLPClassifier(solver='adam', alpha=alpha, hidden_layer_sizes=hls, random_state=1)).run()\n",
    "summarise_run(\"mlp1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "if do_run(\"mlp1\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"mlp1\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, xgboost\", \n",
    "                                               MLPClassifier(solver='adam', random_state=1), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"mlp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "if do_run(\"mlp1\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"mlp1\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, xgboost\", \n",
    "                                               MLPClassifier(solver='saga', random_state=1), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"mlp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if do_run(\"log_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"log_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, logit\", \n",
    "                                               SGDClassifier(loss=\"log\"), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"log_llh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_run(\"pct_llh\"):\n",
    "    for k in [100, 1000, 10000, 100000]:\n",
    "         print(k, LLHbasedFeatures(\"pct_llh\", X_training, y_training, X_dev, y_dev, \n",
    "                                               \"c&w n-grams, llh, perceptron\", \n",
    "                                               SGDClassifier(loss=\"perceptron\"), \n",
    "                                               k=k).run())\n",
    "summarise_run(\"pct_llh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "if do_run(\"bag1\"):\n",
    "    bag1 = TfidfVectorFeatures(\"bag1\", \"c&w n-grams, tfidf, bagging svc\", BaggingClassifier(LinearSVC(), n_estimators=10, max_samples=0.1)).run()\n",
    "summarise_run(\"bag1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#    for alpha in tqdm(10.0 ** -np.arange(1, 7), desc=\"alpha\"):\n",
    "#        for hls in tqdm([(5,2), (5,5), (10,2), (10,5), (50,2), (50,5), (50,10)], desc=\"hls\"):\n",
    "\n",
    "if do_run(\"mlp1\"):\n",
    "    mlp1 = TfidfVectorFeatures(\"mlp1\", \"c&w n-grams, tfidf, mlp adam\", MLPClassifier(solver='adam', alpha=alpha, hidden_layer_sizes=hls, random_state=1)).run()\n",
    "summarise_run(\"mlp1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select k-best character and word n-gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if do_run(\"rfo2\"):\n",
    "    \n",
    "    register_run(\"rfo1\",\n",
    "             \"binary loglikelihood-based vectors on word n-grams, used in a random forest setting\",\n",
    "             ['count'])\n",
    " \n",
    "    print(\"Extracting features\")\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,6),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "    X_training_vec = vectorizer.fit_transform(X_training)\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_training_vect.shape)\n",
    "\n",
    "    print(\"Extracting from test data\")\n",
    "    X_dev_vec = vectorizer.transform(X_dev)\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_dev_vect.shape)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    for k in range(1,100000, 1000):\n",
    "\n",
    "\n",
    "        print(\"Extracting %d best features by a chi-squared test\" % k)\n",
    "        ch2 = SelectKBest(chi2, k=k)\n",
    "        #print(chi2(X_training_vect, y_training))\n",
    "        X_training_vect = ch2.fit_transform(X_training_vec, y_training)\n",
    "        X_dev_vect = ch2.transform(X_dev_vec)\n",
    "#         print(\"n_samples: %d, n_features: %d\" % X_training_vect.shape)\n",
    "#         print(\"n_samples: %d, n_features: %d\" % X_dev_vect.shape)\n",
    "\n",
    "\n",
    "        clf = RandomForestClassifier(max_depth=2)\n",
    "        clf.fit(X_training_vect, y_training)\n",
    "        print(k, clf.score(X_dev_vect, y_dev ))\n",
    "    \n",
    "#     for min_cn in tnrange(1,8, desc=\"min char ngram\"):\n",
    "#         for max_cn in tnrange(min_cn, 8, desc=\"max char ngram\"):\n",
    "\n",
    "#             for min_n in tnrange(1,6, desc=\"min word ngram\"):\n",
    "#                 for max_n in tnrange(min_n,6, desc=\"max word ngram\"):\n",
    "\n",
    "#                     steps = [('char', )),\n",
    "#                              ('words', TfidfVectorizer(analyzer='word', ngram_range=(min_n,max_n),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))\n",
    "#                             ]\n",
    "\n",
    "#                     union = FeatureUnion(steps)\n",
    "\n",
    "#                     pipeline = Pipeline([\n",
    "#                         ('union', union),\n",
    "#                         ('rfo', RandomForestClassifier()),\n",
    "#                     ])\n",
    "\n",
    "#                     model = pipeline.fit(X_training, y_training)\n",
    "#                     print(model)\n",
    "\n",
    "#                     ch2 = SelectKBest(chi2, k=1000)\n",
    "#                     X_training_new = ch2.fit_transform(X_training, y_training)\n",
    "#                     X_dev_new = ch2.transform(X_dev)\n",
    "\n",
    "#                     y_pred = model.predict(X_dev)\n",
    "\n",
    "#                     update_stats(\"rfo2\", y_dev, y_pred, {'min_cn': min_cn, 'max_cn': max_cn, 'min_n': min_n, 'max_n': max_n})\n",
    "\n",
    "summarise_run(\"rfo2\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp\n",
    "mlp is sensitive to feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if do_run(\"xgb1\"):\n",
    "    register_run(\"xgb1\",\n",
    "             \"character and word n-grams, with tf-idf feature vectors, used in a extreme gradient boost setting\",\n",
    "             [])\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "    parameters = {'nthread':[1], #when use hyperthread, xgboost may become slower\n",
    "                  'objective':['binary:logistic'],\n",
    "                  'learning_rate': [0.05], #so called `eta` value\n",
    "                  'max_depth': [6],\n",
    "                  'min_child_weight': [11],\n",
    "                  'silent': [1],\n",
    "                  'subsample': [0.8],\n",
    "                  'colsample_bytree': [0.7],\n",
    "                  'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "                  'missing':[-999],\n",
    "                  'seed': [1337]}\n",
    "\n",
    "    from sklearn.cross_validation import *\n",
    "    clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                       cv=StratifiedKFold(y_training[use], n_folds=5, shuffle=True), \n",
    "                       scoring='accuracy',\n",
    "                       verbose=2, refit=True)\n",
    "\n",
    "    #bst = clf.fit(X_training, y_training)\n",
    "    #xgb_model = xgb.XGBClassifier(nthread=1, objective='binary:logistic', learning_rate=0.05, max_depth=6, min_child_weight=11, missing=-999, n_estimators=1000, subsample=0.8, colsample_bytree=0.7)\n",
    "\n",
    "    #xgb.plot_importance(xgb_model)\n",
    "    #plt.show()\n",
    "    \n",
    "    #update_stats(\"xgb1\", y_dev, y_pred, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
