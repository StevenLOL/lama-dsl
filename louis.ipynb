{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "1. Use binary classification\n",
    "2. Merge training and dev data; use cross-validation\n",
    "\n",
    "3. Use char 6-grams, but also test 7+ is memory and time permits\n",
    "9. Word n-grams (1-3)\n",
    "10. Use term weighting\n",
    "\n",
    "4. nrc uses linear kernel with SVM\n",
    "5. Also consider XGB or LightGBM\n",
    "8. Logistic regression with L2 reg and C=1\n",
    "\n",
    "6. Blacklists/whitelists\n",
    "7. Dimensionality reduction\n",
    "\n",
    "11. Create confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read provisional training material\n",
    "The official data will be released at the end of March. It will probably be the BTI data, so in order to avoid all forms of contamination, we will use a different set. Our data consists of SUBTIEL data, with both Flemish and Netherlandic Dutch subtitles. It requires some preprocessing to convert the files from PAC and STL to SRT. We run this conversion offline, as it also contains some manual steps. And it probably is different from the official data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files are converted to plain text, so we remove all information pertaining to time, colour of the text, and font styles. For the conversion of pac files we run this mess of a grep:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for L in VL NL; do find ./*/${L}/ -iname \"*.pac\" -exec ./unpac {} \\; | grep -v \"\\\"\\| ' *$\\|\\\\$\\|\\&\\|)\\|;\\|%\\|^ .[[:space:]]*$\\|^ ..[[:space:]]*$\\|^ . .[[:space:]]$\\|^[[:space:]]*$\" | sed 's/<\\|>//g' | grep -v \"^[[:space:]]*.$\\|^[[:space:]]*..$\\|^[[:space:]]*.[[:space:]].[[:space:]]*$\\|^[[:space:]]*..[[:space:]]..[[:space:]]*$\\|^[[:space:]]*..[[:space:]].[[:space:]]*$\\|^[[:space:]]*.[[:space:]]..[[:space:]]*$\\^[[:space:]]*.[[:space:]].[[:space:]]\\+.[[:space:]]*$\" | grep -v \"^[[:space:]]*.$\\|^[[:space:]]*..$\\|^[[:space:]]*.[[:space:]].[[:space:]]*$\\|^[[:space:]]*..[[:space:]]..[[:space:]]*$\\|^[[:space:]]*..[[:space:]].[[:space:]]*$\\|^[[:space:]]*.[[:space:]]..[[:space:]]*$\\^[[:space:]]*.[[:space:]].[[:space:]]\\+.[[:space:]]*$\\|BTI\\|Broadcast\\|Title:\\|title:\\|Story:\\|story:\\|Story:\\|TITLE:\\|CONFIG:\\|Config:\\|config:\" > ${L}.unpac; done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The stl files are cleaner; we extract the info with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for L in VL NL; do find ./*/${L}/ -iname \"*.stl\" -printf '%P\\n' -execdir python2 ~/Programming/stl2srt/to_srt.py {} ~/Programming/lama-dsl/data/${L}srt/{} \\; ;  for f in ~/Programming/lama-dsl/data/${L}srt/*.stl; do grep -v \"\\-\\->\\|^[[:space:]]*[[:digit:]]\\+$\\|^[[:space:]]*$\" $f | tail -n +4 ; done > ${L}.unstl; done\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the Flemish data does not have any STL files. Ah well.\n",
    "The first stats:\n",
    "```\n",
    "wc ?L.all   \n",
    "  384631  2770783 15103475 NL.all\n",
    "  296689  2641861 14050991 VL.all\n",
    "```\n",
    "The next step is to run the files through ucto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ucto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c31b9707185a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mucto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ucto'"
     ]
    }
   ],
   "source": [
    "import ucto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ucto_config = \"tokconfig-nld\"\n",
    "vl_tokeniser = ucto.Tokenizer(ucto_config)\n",
    "nl_tokeniser = ucto.Tokenizer(ucto_config)\n",
    "\n",
    "\n",
    "with open('data/VL.all', 'r') as f:\n",
    "    for line in f:\n",
    "        vl_tokeniser.process(line)\n",
    "print(\"--\")\n",
    "vl_text = []\n",
    "current_line = []\n",
    "for token in vl_tokeniser:\n",
    "    current_line.append(str(token))\n",
    "    if token.isendofsentence():\n",
    "        vl_text.append(\" \".join(current_line))\n",
    "        current_line = []\n",
    "print(\"--\")\n",
    "with open('data/NL.all', 'r') as f:\n",
    "    for line in f:\n",
    "        nl_tokeniser.process(line)        \n",
    "print(\"--\")       \n",
    "nl_text = []\n",
    "current_line = []\n",
    "for token in nl_tokeniser:\n",
    "    current_line.append(str(token))\n",
    "    if token.isendofsentence():\n",
    "        nl_text.append(\" \".join(current_line))\n",
    "        current_line = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean length Flemish sentence: \", sum([len(x.split()) for x in vl_text])/len(vl_text))\n",
    "print(\"Mean length Dutch sentence:   \", sum([len(x.split()) for x in vl_text])/len(vl_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any test data, but we will extensively use cross-validation to see our progress (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "## Character $n$-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "vl_cngrams = Counter()\n",
    "for ngram in ngrams(\"hallo dit is een test\", 3):\n",
    "    vl_cngrams[\"\".join(ngram)] += 1\n",
    "\n",
    "nl_cngrams = Counter()\n",
    "for ngram in ngrams(\"hallo dit is een test\", 3):\n",
    "    nl_cngrams[\"\".join(ngram)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word $n$-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vl_cngrams = Counter()\n",
    "for ngram in ngrams(\"hallo dit is een test\".split(), 2):\n",
    "    vl_cngrams[\"\".join(ngram)] += 1\n",
    "\n",
    "nl_cngrams = Counter()\n",
    "for ngram in ngrams(\"hallo dit is een test\".split(), 2):\n",
    "    nl_cngrams[\"\".join(ngram)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
