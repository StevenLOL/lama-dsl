{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "1. Use binary classification\n",
    "2. Merge training and dev data; use cross-validation\n",
    "\n",
    "3. Use char 6-grams, but also test 7+ is memory and time permits\n",
    "9. Word n-grams (1-3)\n",
    "10. Use term weighting\n",
    "\n",
    "4. nrc uses linear kernel with SVM\n",
    "5. Also consider XGB or LightGBM\n",
    "8. Logistic regression with L2 reg and C=1\n",
    "\n",
    "6. Blacklists/whitelists\n",
    "7. Dimensionality reduction\n",
    "\n",
    "11. Create confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read provisional training material\n",
    "The official data will be released at the end of March. It will probably be the BTI data, so in order to avoid all forms of contamination, we will use a different set. Our data consists of SUBTIEL data, with both Flemish and Netherlandic Dutch subtitles. It requires some preprocessing to convert the files from PAC and STL to SRT. We run this conversion offline, as it also contains some manual steps. And it probably is different from the official data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files are converted to plain text, so we remove all information pertaining to time, colour of the text, and font styles. For the conversion of pac files we run this mess of a grep:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for L in VL NL; do find ./*/${L}/ -iname \"*.pac\" -exec ./unpac {} \\; | grep -v \"\\\"\\| ' *$\\|\\\\$\\|\\&\\|)\\|;\\|%\\|^ .[[:space:]]*$\\|^ ..[[:space:]]*$\\|^ . .[[:space:]]$\\|^[[:space:]]*$\" | sed 's/<\\|>//g' | grep -v \"^[[:space:]]*.$\\|^[[:space:]]*..$\\|^[[:space:]]*.[[:space:]].[[:space:]]*$\\|^[[:space:]]*..[[:space:]]..[[:space:]]*$\\|^[[:space:]]*..[[:space:]].[[:space:]]*$\\|^[[:space:]]*.[[:space:]]..[[:space:]]*$\\^[[:space:]]*.[[:space:]].[[:space:]]\\+.[[:space:]]*$\" | grep -v \"^[[:space:]]*.$\\|^[[:space:]]*..$\\|^[[:space:]]*.[[:space:]].[[:space:]]*$\\|^[[:space:]]*..[[:space:]]..[[:space:]]*$\\|^[[:space:]]*..[[:space:]].[[:space:]]*$\\|^[[:space:]]*.[[:space:]]..[[:space:]]*$\\^[[:space:]]*.[[:space:]].[[:space:]]\\+.[[:space:]]*$\\|BTI\\|Broadcast\\|Title:\\|title:\\|Story:\\|story:\\|Story:\\|TITLE:\\|CONFIG:\\|Config:\\|config:\" > ${L}.unpac; done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The stl files are cleaner; we extract the info with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for L in VL NL; do find ./*/${L}/ -iname \"*.stl\" -printf '%P\\n' -execdir python2 ~/Programming/stl2srt/to_srt.py {} ~/Programming/lama-dsl/data/${L}srt/{} \\; ;  for f in ~/Programming/lama-dsl/data/${L}srt/*.stl; do grep -v \"\\-\\->\\|^[[:space:]]*[[:digit:]]\\+$\\|^[[:space:]]*$\" $f | tail -n +4 ; done > ${L}.unstl; done\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the Flemish data does not have any STL files. Ah well.\n",
    "The first stats:\n",
    "```\n",
    "wc ?L.all   \n",
    "  384631  2770783 15103475 NL.all\n",
    "  296689  2641861 14050991 VL.all\n",
    "```\n",
    "The next step is to run the files through ucto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucto\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PICKLE RICK!!!\n"
     ]
    }
   ],
   "source": [
    "ucto_config = \"tokconfig-nld\"\n",
    "\n",
    "vl_text = []\n",
    "try:\n",
    "    with open('data/VL.all.pickle', 'rb') as f:\n",
    "        vl_text = pickle.load(f)\n",
    "except IOError:    \n",
    "    vl_tokeniser = ucto.Tokenizer(ucto_config)\n",
    "    with open('data/VL.all', 'r') as f:\n",
    "        for line in f:\n",
    "            vl_tokeniser.process(line)\n",
    "    print(\"All Flemish data has been tokenised.\")\n",
    "\n",
    "    current_line = []\n",
    "    for token in vl_tokeniser:\n",
    "        current_line.append(str(token))\n",
    "        if token.isendofsentence():\n",
    "            vl_text.append(\" \".join(current_line))\n",
    "            current_line = []\n",
    "    print(\"All Flemish data has been converted to sentences.\")\n",
    "    \n",
    "    with open('data/VL.all.pickle', 'wb') as f:\n",
    "        pickle.dump(vl_text, f, pickle.HIGHEST_PROTOCOL)    \n",
    "    print(\"All Flemish sentences have been written to a pickle.\")\n",
    "\n",
    "nl_text = []\n",
    "try:\n",
    "    with open('data/NL.all.pickle', 'rb') as f:\n",
    "        nl_text = pickle.load(f)\n",
    "except IOError:\n",
    "    nl_tokeniser = ucto.Tokenizer(ucto_config)\n",
    "    with open('data/NL.all', 'r') as f:\n",
    "        for line in f:\n",
    "            nl_tokeniser.process(line)        \n",
    "    print(\"All Netherlandic data has been tokenised.\")     \n",
    "\n",
    "    current_line = []\n",
    "    for token in nl_tokeniser:\n",
    "        current_line.append(str(token))\n",
    "        if token.isendofsentence():\n",
    "            nl_text.append(\" \".join(current_line))\n",
    "            current_line = []\n",
    "    print(\"All Netherlandic data has been converted to sentences.\")\n",
    "    \n",
    "    with open('data/NL.all.pickle', 'wb') as f:\n",
    "        pickle.dump(nl_text, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"All Netherlandic sentences have been written to a pickle.\")\n",
    "print(\"PICKLE RICK!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_text = vl_text + nl_text\n",
    "xl_labels = ['vl'] * len(vl_text) + ['nl'] * len(nl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length Flemish sentence:  7.059401465519209\n",
      "Mean length Dutch sentence:    6.848851963185248\n",
      "Mean length all sentences:     6.948807228693701\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean length Flemish sentence: \", sum([len(x.split()) for x in vl_text])/len(vl_text))\n",
    "print(\"Mean length Dutch sentence:   \", sum([len(x.split()) for x in nl_text])/len(nl_text))\n",
    "print(\"Mean length all sentences:    \", sum([len(x.split()) for x in xl_text])/len(xl_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any test data, but we will extensively use cross-validation to see our progress (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('char', CountVectorizer(analyzer='char', ngram_range=(3,3))),\n",
    "         ('words', CountVectorizer(analyzer='word', ngram_range=(3,3),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"))]\n",
    "\n",
    "union = FeatureUnion(steps)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('union', union),\n",
    "    ('svc', SVC(kernel='linear')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Waar gaan we vandaag lunchen ?', 'Zal ik naar de galerie komen ?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "small_text = vl_text[0:20] + nl_text[0:20]\n",
    "small_labels = ['vl'] * 20 + ['nl'] * 20\n",
    "print(len(small_text))\n",
    "print(len(small_labels))\n",
    "\n",
    "small_test = vl_text[-5:] + nl_text[-5:]\n",
    "small_testl = ['vl'] * 5 + ['nl'] * 5\n",
    "print(len(small_test))\n",
    "print(len(small_testl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model on data\n",
      "Predict the labels on test data\n",
      "Score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fitting model on data\")\n",
    "prediction = pipeline.fit(small_text, small_labels)\n",
    "print(\"Predict the labels on test data\")\n",
    "prediction = pipeline.predict(small_test)\n",
    "print(\"Score\")\n",
    "pipeline.score(small_test, small_testl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 char__ a \n",
      "1 char__ an\n",
      "2 char__ in\n",
      "3 char__ is\n",
      "4 char__ lo\n",
      "5 char__ or\n",
      "6 char__ se\n",
      "7 char__ so\n",
      "8 char__ te\n",
      "9 char__ th\n",
      "10 char__ wh\n",
      "11 char__a t\n",
      "12 char__and\n",
      "13 char__ano\n",
      "14 char__ats\n",
      "15 char__d t\n",
      "16 char__e w\n",
      "17 char__ee \n",
      "18 char__enc\n",
      "19 char__ent\n",
      "20 char__er \n",
      "21 char__ere\n",
      "22 char__est\n",
      "23 char__eth\n",
      "24 char__ets\n",
      "25 char__ger\n",
      "26 char__hat\n",
      "27 char__her\n",
      "28 char__hin\n",
      "29 char__his\n",
      "30 char__in \n",
      "31 char__ing\n",
      "32 char__is \n",
      "33 char__let\n",
      "34 char__lon\n",
      "35 char__met\n",
      "36 char__n t\n",
      "37 char__nce\n",
      "38 char__nd \n",
      "39 char__nge\n",
      "40 char__not\n",
      "41 char__nte\n",
      "42 char__ome\n",
      "43 char__ong\n",
      "44 char__or \n",
      "45 char__oth\n",
      "46 char__r l\n",
      "47 char__r s\n",
      "48 char__s a\n",
      "49 char__s i\n",
      "50 char__s s\n",
      "51 char__see\n",
      "52 char__sen\n",
      "53 char__som\n",
      "54 char__st \n",
      "55 char__t o\n",
      "56 char__ten\n",
      "57 char__tes\n",
      "58 char__the\n",
      "59 char__thi\n",
      "60 char__ts \n",
      "61 char__wha\n",
      "62 words__a test or\n",
      "63 words__and this is\n",
      "64 words__another longer sentence\n",
      "65 words__is a test\n",
      "66 words__is another longer\n",
      "67 words__lets see whats\n",
      "68 words__see whats in\n",
      "69 words__test or something\n",
      "70 words__this is a\n",
      "71 words__this is another\n",
      "72 words__whats in there\n"
     ]
    }
   ],
   "source": [
    "for x,y in enumerate(union.get_feature_names()):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4,5,6][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [307589 307590 307591 ... 922764 922765 922766] | test: [     0      1      2 ... 307586 307587 307588]\n",
      "Train: [     0      1      2 ... 922764 922765 922766] | test: [307589 307590 307591 ... 615175 615176 615177]\n",
      "Train: [     0      1      2 ... 615175 615176 615177] | test: [615178 615179 615180 ... 922764 922765 922766]\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits=3)\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(xl_text):\n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "\n",
    "    param_grid = dict(features__univ_select__k=[1, 2],\n",
    "                      svm__C=[0.1, 1, 10])\n",
    "    #grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\n",
    "    #grid_search.fit(alltrainingmaterial[train], alllabels[train]).score(alltrainingmaterial[test], alllabels[test])\n",
    "    #    for train, test in k_fold.split(alltrainingmaterial)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
